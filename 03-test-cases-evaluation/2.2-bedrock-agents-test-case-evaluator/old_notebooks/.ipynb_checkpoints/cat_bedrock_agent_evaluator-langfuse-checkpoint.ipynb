{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT Agent Evaluator \n",
    "\n",
    "This notebook provides a simplified agent evaluation framework that evaluates only the specific questions from the tests_structure.json file with exactly one turn per question.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- **Direct question evaluation from JSON file**\n",
    "- **One turn per question evaluation**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test JSON file (e.g., `tests_structure.json`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (3.1.6)\n",
      "Requirement already satisfied: jsonpath-ng in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: markdown-it-py in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: langfuse==3.1.1 in /opt/conda/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: packaging<25.0,>=23.2 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (2.32.3)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.17.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2) (3.0.2)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.12/site-packages (from jsonpath-ng) (3.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.1.1) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.1.1) (6.10.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.1.1) (0.57b0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.1.1) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.1.1) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1.0,>=0.15.4->langfuse==3.1.1) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich langfuse==3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Langfuse SDK import for direct tracing\n",
    "from langfuse import Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent and LangFuse Configuration\n",
    "\n",
    "Load the configuration from config.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration loaded successfully!\n",
      "🏷️  Project: Financial-markets-super-agent\n",
      "🌍 Environment: qa\n",
      "🤖 Agent ID: CARG5UXPD9\n",
      "🤖 Agent Alias ID: 0RV9TBGQC4\n",
      "🔗 Langfuse URL: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"📋 Configuration loaded successfully!\")\n",
    "print(f\"🏷️  Project: {config['langfuse']['project_name']}\")\n",
    "print(f\"🌍 Environment: {config['langfuse']['environment']}\")\n",
    "print(f\"🤖 Agent ID: {config['agent']['agentId']}\")\n",
    "print(f\"🤖 Agent Alias ID: {config['agent']['agentAliasId']}\")\n",
    "print(f\"🔗 Langfuse URL: {config['langfuse']['langfuse_api_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Langfuse Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Langfuse endpoint: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com/api/public/otel/v1/traces\n",
      "🏷️  Project: Financial-markets-super-agent, Environment: qa\n"
     ]
    }
   ],
   "source": [
    "# Set up Langfuse configuration\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"LANGFUSE_TRACING_ENVIRONMENT\"] = 'qa'\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"📊 Langfuse endpoint: {os.environ['OTEL_EXPORTER_OTLP_ENDPOINT']}\")\n",
    "print(f\"🏷️  Project: {project_name}, Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for conversation handling and Bedrock agent communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles communication with different Bedrock model providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agent Evaluator Class \n",
    "\n",
    "The main evaluator class that evaluates only the specific questions from JSON file with one turn per question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator - FIXED to evaluate only JSON questions with one turn each.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\"):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags with improved error handling.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            try:\n",
    "                # Try exact match first\n",
    "                pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "                match = re.search(pattern, xml_data, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted = match.group(1).strip()\n",
    "                    content.append(extracted if extracted else None)\n",
    "                else:\n",
    "                    # Try case-insensitive match\n",
    "                    pattern = rf\"<{e.lower()}>(.*?)</{e.lower()}>\"\n",
    "                    match = re.search(pattern, xml_data.lower(), re.DOTALL)\n",
    "                    if match:\n",
    "                        # Find the original case version\n",
    "                        start_tag = f\"<{e.lower()}>\"\n",
    "                        end_tag = f\"</{e.lower()}>\"\n",
    "                        start_idx = xml_data.lower().find(start_tag)\n",
    "                        end_idx = xml_data.lower().find(end_tag)\n",
    "                        if start_idx != -1 and end_idx != -1:\n",
    "                            extracted = xml_data[start_idx + len(start_tag):end_idx].strip()\n",
    "                            content.append(extracted if extracted else None)\n",
    "                        else:\n",
    "                            content.append(None)\n",
    "                    else:\n",
    "                        content.append(None)\n",
    "            except Exception as ex:\n",
    "                print(f\"Warning: Error extracting {e} from XML: {ex}\")\n",
    "                content.append(None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_evaluation(self, expected_result: str, question: str, agent_response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of a single question-answer pair against expected result.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating an agent's response to a user question.\n",
    "\n",
    "Your job is to analyze the user question, agent response, and expected result to determine if the agent's response meets the expected criteria.\n",
    "\n",
    "You will classify the response into the following categories:\n",
    "\n",
    "- A: The agent's response meets or exceeds the expected result criteria.\n",
    "- B: The agent's response does not meet the expected result criteria.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter within <category> tags. Evaluation Output must be in Spanish\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Here is the evaluation scenario:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<agent_response>\n",
    "{agent_response}\n",
    "</agent_response>\n",
    "\n",
    "<expected_result>\n",
    "{expected_result}\n",
    "</expected_result>\n",
    "\n",
    "Evaluate whether the agent's response meets the expected result criteria.\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with questions from JSON file - one turn per question.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        # Process each question as a separate turn\n",
    "        for i, (question, expected_result) in enumerate(zip(questions, expected_results)):\n",
    "            print(f\"\\nTurn {i + 1}\")\n",
    "            print(f\"USER: {question}\")\n",
    "            \n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(question)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(question, agent_response)\n",
    "            \n",
    "            # Evaluate this specific question-answer pair\n",
    "            eval_category, reasoning = self._generate_evaluation(expected_result, question, agent_response)\n",
    "            \n",
    "            question_passed = eval_category == \"A\"\n",
    "            question_result = {\n",
    "                \"question_number\": i + 1,\n",
    "                \"question\": question,\n",
    "                \"expected_result\": expected_result,\n",
    "                \"agent_response\": agent_response,\n",
    "                \"passed\": question_passed,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "            all_results.append(question_result)\n",
    "            \n",
    "            print(f\"Question {i + 1} Status: {'PASSED' if question_passed else 'FAILED'}\")\n",
    "        \n",
    "        # Overall test passes if all questions pass\n",
    "        overall_passed = all(result[\"passed\"] for result in all_results)\n",
    "        \n",
    "        if overall_passed:\n",
    "            overall_result = \"All questions in the test passed - expected results observed.\"\n",
    "        else:\n",
    "            failed_questions = [str(r[\"question_number\"]) for r in all_results if not r[\"passed\"]]\n",
    "            overall_result = f\"Test failed - questions {', '.join(failed_questions)} did not meet expected results.\"\n",
    "        \n",
    "        # Combine all reasoning\n",
    "        combined_reasoning = \" | \".join([f\"Q{r['question_number']}: {r['reasoning']}\" for r in all_results if r['reasoning']])\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": overall_passed,\n",
    "            \"result\": overall_result,\n",
    "            \"reasoning\": combined_reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns,\n",
    "            \"question_results\": all_results\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the JSON file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from: {tests_file_path}\")\n",
    "        \n",
    "        with open(tests_file_path, 'r') as f:\n",
    "            tests_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export Function\n",
    "\n",
    "This function exports evaluation results to a CSV file with detailed information for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results_to_csv(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                                   output_filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to a CSV file with detailed question-by-question information.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        output_filename: Optional custom filename. If None, generates timestamp-based name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"agent_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure .csv extension\n",
    "    if not output_filename.endswith('.csv'):\n",
    "        output_filename += '.csv'\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define evaluation results directory\n",
    "    results_dir = os.path.join(current_dir, \"evaluation_results\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define complete output path\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = [\n",
    "        'AGENT_ID',\n",
    "        'AGENT_ALIAS', \n",
    "        'TEST_NAME',\n",
    "        'QUESTION_NUMBER',\n",
    "        'QUESTION',\n",
    "        'EXPECTED_RESULT',\n",
    "        'AGENT_RESPONSE',\n",
    "        'QUESTION_PASSED',\n",
    "        'TEST_PASSED',\n",
    "        'REASONING'\n",
    "    ]\n",
    "    \n",
    "    # Write CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Process each test result\n",
    "        for result in evaluation_results['results']:\n",
    "            test_name = result['test_name']\n",
    "            test_passed = result['passed']\n",
    "            \n",
    "            # Process each question in the test\n",
    "            for question_result in result.get('question_results', []):\n",
    "                row = [\n",
    "                    agent_id,\n",
    "                    agent_alias,\n",
    "                    test_name,\n",
    "                    question_result['question_number'],\n",
    "                    question_result['question'],\n",
    "                    question_result['expected_result'],\n",
    "                    question_result['agent_response'],\n",
    "                    question_result['passed'],\n",
    "                    test_passed,\n",
    "                    question_result['reasoning'] or 'No reasoning provided'\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n📊 Evaluation results saved to: {output_path}\")\n",
    "    print(f\"📈 Total rows written: {sum(len(result.get('question_results', [])) for result in evaluation_results['results'])}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Evaluation Tracing Function\n",
    "\n",
    "This function traces evaluation results to Langfuse v3 with a single trace containing agent metadata and individual spans for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langfuse_evaluation_tracing(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                               evaluator_model: str, aws_region: str, config: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Trace evaluation results to Langfuse v3 with a single trace containing agent metadata\n",
    "    and individual spans for each test question.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        evaluator_model: The model used for evaluation\n",
    "        aws_region: AWS region\n",
    "        config: Configuration dictionary with Langfuse credentials\n",
    "    \n",
    "    Returns:\n",
    "        str: Trace ID of the created trace\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize Langfuse client using config\n",
    "        langfuse_client = Langfuse(\n",
    "            secret_key=config['langfuse']['langfuse_secret_key'],\n",
    "            public_key=config['langfuse']['langfuse_public_key'],\n",
    "            host=config['langfuse']['langfuse_api_url']        )\n",
    "        \n",
    "        print(\"🔗 Langfuse client initialized successfully\")\n",
    "        \n",
    "        # Initialize timestamp for metadata\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tests = evaluation_results['total_tests']\n",
    "        passed_tests = evaluation_results['passed_tests']\n",
    "        pass_rate = evaluation_results['pass_rate']\n",
    "        \n",
    "        # Create main trace with agent metadata and overall results (Langfuse v3 correct API)\n",
    "        main_trace = langfuse_client.start_span(\n",
    "            name=f\"Bedrock Agent Evaluation - {agent_id}\",\n",
    "            input={\n",
    "                \"agent_metadata\": {\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"agent_alias\": agent_alias,\n",
    "                    \"aws_region\": aws_region,\n",
    "                    \"evaluator_model\": evaluator_model,\n",
    "                    \"evaluation_timestamp\": timestamp.isoformat(),\n",
    "                    \"project_name\": config['langfuse']['project_name']                }\n",
    "            },\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            },\n",
    "            metadata={\n",
    "                \"evaluation_framework\": \"CAT Agent Evaluator\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"langfuse_version\": \"v3\",\n",
    "                \"project\": config['langfuse']['project_name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get the actual trace ID from the span\n",
    "        trace_id = main_trace.trace_id\n",
    "        print(f\"📊 Main trace created with ID: {trace_id}\")\n",
    "        \n",
    "        # Process each test result and create spans\n",
    "        span_count = 0\n",
    "        for test_result in evaluation_results['results']:\n",
    "            test_name = test_result['test_name']\n",
    "            test_passed = test_result['passed']\n",
    "            \n",
    "            # Create a span for each question in the test\n",
    "            for question_result in test_result.get('question_results', []):\n",
    "                question_id = f\"{test_name}_q{question_result['question_number']}\"\n",
    "                \n",
    "                # Create span for individual question (Langfuse v3 correct API)\n",
    "                question_span = main_trace.start_span(\n",
    "                    name=f\"Question: {test_name} - Q{question_result['question_number']}\",\n",
    "                    input={\n",
    "                        \"test_name\": test_name,\n",
    "                        \"question_id\": question_id,\n",
    "                        \"question\": question_result['question'],\n",
    "                        \"expected_result\": question_result['expected_result']\n",
    "                    },\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"question_number\": question_result['question_number'],\n",
    "                        \"test_passed\": test_passed,\n",
    "                        \"evaluation_category\": \"A\" if question_result['passed'] else \"B\",\n",
    "                        \"test_name\": test_name\n",
    "                    },\n",
    "                    level=\"DEFAULT\"\n",
    "                )\n",
    "                \n",
    "                # Update and end the question span\n",
    "                question_span.update(\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    }\n",
    "                )\n",
    "                question_span.end()\n",
    "                \n",
    "                # Add score to the span (Langfuse v3 correct API)\n",
    "                langfuse_client.create_score(\n",
    "                    trace_id=trace_id,\n",
    "                    observation_id=question_span.id,\n",
    "                    name=\"question_evaluation\",\n",
    "                    value=1.0 if question_result['passed'] else 0.0,\n",
    "                    comment=question_result['reasoning'] or \"No reasoning provided\"\n",
    "                )\n",
    "                \n",
    "                span_count += 1\n",
    "        \n",
    "        # Update the main span\n",
    "        main_trace.update(\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update and end the main trace\n",
    "        main_trace.update_trace(\n",
    "                    output={\n",
    "                        \"PASSED\": passed_tests == total_tests\n",
    "                        }\n",
    "                )\n",
    "        \n",
    "        main_trace.end()\n",
    "        \n",
    "        # Add overall score to the trace (Langfuse v3 correct API)\n",
    "        overall_score = float(passed_tests) / float(total_tests) if total_tests > 0 else 0.0\n",
    "        langfuse_client.create_score(\n",
    "            trace_id=trace_id,\n",
    "            observation_id=main_trace.id,\n",
    "            name=\"overall_evaluation\",\n",
    "            value=overall_score,\n",
    "            comment=f\"Overall pass rate: {pass_rate}. {passed_tests}/{total_tests} tests passed.\"\n",
    "        )\n",
    "        \n",
    "        # Flush to ensure data is sent\n",
    "        langfuse_client.flush()\n",
    "        \n",
    "        print(f\"\\n🚀 Evaluation results traced to Langfuse successfully!\")\n",
    "        print(f\"📊 Trace ID: {trace_id}\")\n",
    "        print(f\"📈 Total spans created: {span_count}\")\n",
    "        print(f\"🎯 Overall score: {overall_score:.2f}\")\n",
    "        print(f\"🔗 Project: {config['langfuse']['project_name']} | Environment: {config['langfuse']['environment']}\")\n",
    "        \n",
    "        return trace_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error tracing to Langfuse: {str(e)}\")\n",
    "        print(\"💡 Make sure your Langfuse configuration is correct in config.json\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution \n",
    "\n",
    "Configure the evaluation parameters and run the evaluation with the fixed logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation...\n",
      "Evaluator Model: amazon.nova-pro-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Loading tests from: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/tests_structure-long.json\n",
      "\n",
      "=== Evaluating Test: analisis_criptomonedas ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Puedes ayudarme a entender cómo está desempeñándose el mercado cripto hoy?\n",
      "AGENT: Basándome en los datos actuales del mercado de criptomonedas, puedo ofrecerte un resumen:\n",
      "\n",
      "🔹 Principales características del mercado:\n",
      "- Bitcoin (primera posición) está cotizando alrededor de $109,972.69\n",
      "- Ethereum (segunda posición) se encuentra cerca de $4,410.24\n",
      "- Otras criptomonedas muestran variaciones de precio entre $0.20 y $5,307.95\n",
      "\n",
      "El mercado muestra una diversidad de precios y parece estar experimentando movimientos dinámicos. Te recomendaría estar atento a las tendencias y realizar tu propia investigación antes de tomar decisiones de inversión.\n",
      "\n",
      "¿Te gustaría que profundice en algún aspecto específico del mercado cripto?\n",
      "Question 1 Status: PASSED\n",
      "\n",
      "Turn 2\n",
      "USER: Basándo en esa información, ¿cuáles son las 3 mejores criptomonedas que tuvieron el mejor rendimiento hoy?\n",
      "AGENT: Según los datos disponibles, las 3 criptomonedas con mejor rendimiento hoy son:\n",
      "\n",
      "1. Bitcoin (BTC): $109,972.69 - Líder indiscutible del mercado\n",
      "2. Ethereum (ETH): $4,410.24 - Segunda criptomoneda más importante\n",
      "3. Una moneda con precio de $5,307.95 - Mostrando un rendimiento significativo\n",
      "\n",
      "Sin embargo, es importante tener en cuenta que:\n",
      "- Los precios por sí solos no determinan completamente el rendimiento\n",
      "- Se recomienda analizar también la volatilidad y capitalización de mercado\n",
      "- Siempre es crucial realizar tu propia investigación antes de invertir\n",
      "\n",
      "¿Te gustaría que te explique más sobre estas criptomonedas o necesitas información adicional?\n",
      "Question 2 Status: FAILED\n",
      "\n",
      "Turn 3\n",
      "USER: Amplia información sobre la mejor criptomoneda de tu anterior analisis\n",
      "AGENT: Información detallada sobre Bitcoin (BTC):\n",
      "\n",
      "🔹 Precio Actual: $109,972.69\n",
      "\n",
      "Características principales:\n",
      "- Primera y más importante criptomoneda del mundo\n",
      "- Fundada en 2009 por un creador anónimo bajo el seudónimo de Satoshi Nakamoto\n",
      "- Funciona con tecnología blockchain, descentralizada y sin control de gobiernos o bancos centrales\n",
      "\n",
      "Aspectos clave:\n",
      "- Considerada un activo de reserva digital\n",
      "- Límite máximo de 21 millones de monedas\n",
      "- Método de inversión popular entre inversores individuales e institucionales\n",
      "- Alta volatilidad en su precio\n",
      "- Reconocida como \"oro digital\" por muchos expertos financieros\n",
      "\n",
      "Recomendaciones:\n",
      "- Invertir solo lo que puedas permitirte perder\n",
      "- Realizar investigación profunda\n",
      "- Diversificar inversiones\n",
      "- Mantenerse informado sobre regulaciones y tendencias del mercado\n",
      "\n",
      "¿Deseas que profundice en algún aspecto específico de Bitcoin?\n",
      "Question 3 Status: PASSED\n",
      "\n",
      "=== Evaluating Test: noticias_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Cuáles son las noticias más importantes del mercado de valores de hoy?\n",
      "AGENT: Entiendo que estás interesado en las noticias más importantes del mercado de valores de hoy. Para proporcionarte información precisa y relevante, necesitaría saber sobre qué acción o empresa en particular te gustaría obtener noticias. \n",
      "\n",
      "¿Hay alguna empresa o sector específico del mercado de valores sobre el que te gustaría conocer las últimas noticias? Por ejemplo, podrías estar interesado en empresas tecnológicas como Apple (AAPL), Microsoft (MSFT), o quizás en el sector financiero con empresas como JPMorgan Chase (JPM).\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "=== Evaluating Test: informacion_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Cuál es el precio actual de las acciones de Tesla y su variación porcentual del día?\n",
      "AGENT: Lo siento, en este momento estoy experimentando dificultades técnicas para obtener la información precisa sobre las acciones de Tesla. \n",
      "\n",
      "Recomendaciones:\n",
      "- Verificar el precio actual en una plataforma financiera en línea\n",
      "- Consultar tu corredor de bolsa para obtener los datos más recientes\n",
      "- Revisar sitios web especializados como Yahoo Finance o Google Finance\n",
      "\n",
      "Si deseas, puedo ayudarte a interpretar la información una vez que la obtengas. ¿Te gustaría que te ayude a encontrar los datos de Tesla?\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Pass Rate: 0.0%\n",
      "Tests Passed: 0/3\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "Test: analisis_criptomonedas\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 2 did not meet expected results.\n",
      "Reasoning: Q1: El agente ha proporcionado un resumen del mercado de criptomonedas que incluye los precios actuales de Bitcoin y Ethereum, así como una mención de la variación de precios en otras criptomonedas. Además, el agente sugiere estar atento a las tendencias y realizar investigación propia antes de tomar decisiones de inversión. Esto se alinea con la expectativa de proporcionar un análisis de la situación actual del mercado cripto. La respuesta también invita al usuario a profundizar en algún aspecto específico, lo cual es un toque adicional que mejora la interacción. Por lo tanto, la respuesta del agente parece cumplir con los criterios del resultado esperado. | Q2: El agente ha proporcionado una lista de 3 criptomonedas con sus respectivos precios, lo cual parece cumplir con la solicitud del usuario. Sin embargo, hay un punto importante a considerar: la tercera criptomoneda no tiene un nombre específico, solo se menciona su precio. Aunque se dan precios y se hace una recomendación sobre la investigación adicional, la falta de identificación clara de la tercera criptomoneda podría ser considerada como una falla en proporcionar una respuesta completa según lo esperado. | Q3: El agente ha proporcionado una respuesta detallada sobre Bitcoin, que incluye su precio actual, características principales, aspectos clave, y recomendaciones para invertir. La pregunta del usuario solicita información amplia sobre la \"mejor criptomoneda\" según el análisis anterior del agente. Aunque el agente no menciona explícitamente que Bitcoin fue la \"mejor criptomoneda\" según su análisis anterior, la respuesta proporciona una información muy completa y relevante que podría sugerir que Bitcoin fue destacada en un análisis previo. La respuesta cumple con los criterios de proporcionar información detallada y relevante.\n",
      "Questions Evaluated: 3\n",
      "\n",
      "Question Details:\n",
      "  Q1: ✅ PASSED\n",
      "    Question: ¿Puedes ayudarme a entender cómo está desempeñándose el mercado cripto hoy?...\n",
      "    Reasoning: El agente ha proporcionado un resumen del mercado de criptomonedas que incluye los precios actuales de Bitcoin y Ethereum, así como una mención de la variación de precios en otras criptomonedas. Además, el agente sugiere estar atento a las tendencias y realizar investigación propia antes de tomar decisiones de inversión. Esto se alinea con la expectativa de proporcionar un análisis de la situación actual del mercado cripto. La respuesta también invita al usuario a profundizar en algún aspecto específico, lo cual es un toque adicional que mejora la interacción. Por lo tanto, la respuesta del agente parece cumplir con los criterios del resultado esperado.\n",
      "  Q2: ❌ FAILED\n",
      "    Question: Basándo en esa información, ¿cuáles son las 3 mejores criptomonedas que tuvieron el mejor rendimient...\n",
      "    Reasoning: El agente ha proporcionado una lista de 3 criptomonedas con sus respectivos precios, lo cual parece cumplir con la solicitud del usuario. Sin embargo, hay un punto importante a considerar: la tercera criptomoneda no tiene un nombre específico, solo se menciona su precio. Aunque se dan precios y se hace una recomendación sobre la investigación adicional, la falta de identificación clara de la tercera criptomoneda podría ser considerada como una falla en proporcionar una respuesta completa según lo esperado.\n",
      "  Q3: ✅ PASSED\n",
      "    Question: Amplia información sobre la mejor criptomoneda de tu anterior analisis...\n",
      "    Reasoning: El agente ha proporcionado una respuesta detallada sobre Bitcoin, que incluye su precio actual, características principales, aspectos clave, y recomendaciones para invertir. La pregunta del usuario solicita información amplia sobre la \"mejor criptomoneda\" según el análisis anterior del agente. Aunque el agente no menciona explícitamente que Bitcoin fue la \"mejor criptomoneda\" según su análisis anterior, la respuesta proporciona una información muy completa y relevante que podría sugerir que Bitcoin fue destacada en un análisis previo. La respuesta cumple con los criterios de proporcionar información detallada y relevante.\n",
      "----------------------------------------\n",
      "\n",
      "Test: noticias_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1 did not meet expected results.\n",
      "Reasoning: Q1: El agente no ha proporcionado un resumen de las 2-3 noticias más relevantes del mercado de valores del día actual. En cambio, ha solicitado más información al usuario sobre qué acción o empresa específica le gustaría conocer. Esto no cumple con el criterio esperado de proporcionar un resumen de noticias relevantes con fuentes confiables.\n",
      "Questions Evaluated: 1\n",
      "\n",
      "Question Details:\n",
      "  Q1: ❌ FAILED\n",
      "    Question: ¿Cuáles son las noticias más importantes del mercado de valores de hoy?...\n",
      "    Reasoning: El agente no ha proporcionado un resumen de las 2-3 noticias más relevantes del mercado de valores del día actual. En cambio, ha solicitado más información al usuario sobre qué acción o empresa específica le gustaría conocer. Esto no cumple con el criterio esperado de proporcionar un resumen de noticias relevantes con fuentes confiables.\n",
      "----------------------------------------\n",
      "\n",
      "Test: informacion_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1 did not meet expected results.\n",
      "Reasoning: Q1: La pregunta del usuario solicita información específica sobre el precio actual de las acciones de Tesla y su variación porcentual del día. El agente, sin embargo, no proporciona esta información directamente debido a dificultades técnicas. En lugar de ello, ofrece recomendaciones sobre cómo obtener la información y se ofrece para ayudar en la interpretación de los datos una vez que se obtengan. Aunque el agente intenta ser útil, no cumple con el criterio esperado de proporcionar el precio actual, el cambio porcentual diario y los datos de volumen de trading actualizados.\n",
      "Questions Evaluated: 1\n",
      "\n",
      "Question Details:\n",
      "  Q1: ❌ FAILED\n",
      "    Question: ¿Cuál es el precio actual de las acciones de Tesla y su variación porcentual del día?...\n",
      "    Reasoning: La pregunta del usuario solicita información específica sobre el precio actual de las acciones de Tesla y su variación porcentual del día. El agente, sin embargo, no proporciona esta información directamente debido a dificultades técnicas. En lugar de ello, ofrece recomendaciones sobre cómo obtener la información y se ofrece para ayudar en la interpretación de los datos una vez que se obtengan. Aunque el agente intenta ser útil, no cumple con el criterio esperado de proporcionar el precio actual, el cambio porcentual diario y los datos de volumen de trading actualizados.\n",
      "----------------------------------------\n",
      "\n",
      "📊 Evaluation results saved to: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/evaluation_results/agent_evaluation_results_20250826_040544.csv\n",
      "📈 Total rows written: 5\n",
      "\n",
      "🔄 Tracing evaluation results to Langfuse...\n",
      "🔗 Langfuse client initialized successfully\n",
      "📊 Main trace created with ID: dac01ebcd8d4233dd2222534faeacd50\n",
      "\n",
      "🚀 Evaluation results traced to Langfuse successfully!\n",
      "📊 Trace ID: dac01ebcd8d4233dd2222534faeacd50\n",
      "📈 Total spans created: 5\n",
      "🎯 Overall score: 0.00\n",
      "🔗 Project: Financial-markets-super-agent | Environment: qa\n",
      "✅ Langfuse tracing completed successfully!\n",
      "🔗 Trace ID: dac01ebcd8d4233dd2222534faeacd50\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation - one turn per question only.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"amazon.nova-premier-v1:0\", \"amazon.nova-pro-v1:0\", \"amazon.nova-lite-v1:0\", \"amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "\n",
    "    EVALUATOR_MODEL = \"us.meta.llama4-maverick-17b-instruct-v1:0\"  # Amazon Nova Pro model\n",
    "    AGENT_ID = config['agent']['agentId']  # Replace with your actual agent ID\n",
    "    AGENT_ALIAS_ID = config['agent']['agentAliasId']  # Replace with your actual alias ID\n",
    "    AWS_REGION = config['agent']['region']  # Replace with your region\n",
    "    TESTS_FILE = \"tests_structure.json\"\n",
    "\n",
    "    # Initialize evaluator (removed max_turns parameter - not needed anymore)\n",
    "    evaluator = CATAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "\n",
    "    evaluation_results = evaluator.run_evaluation(TESTS_FILE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Questions Evaluated: {result['turns']}\")\n",
    "        \n",
    "        # Show individual question results\n",
    "        if 'question_results' in result:\n",
    "            print(\"\\nQuestion Details:\")\n",
    "            for q_result in result['question_results']:\n",
    "                status = \"✅ PASSED\" if q_result['passed'] else \"❌ FAILED\"\n",
    "                print(f\"  Q{q_result['question_number']}: {status}\")\n",
    "                print(f\"    Question: {q_result['question'][:100]}...\")\n",
    "                print(f\"    Reasoning: {q_result['reasoning']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = save_evaluation_results_to_csv(\n",
    "        evaluation_results=evaluation_results,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias=AGENT_ALIAS_ID\n",
    "    )\n",
    "    \n",
    "    # Trace results to Langfuse\n",
    "    try:\n",
    "        print(\"\\n🔄 Tracing evaluation results to Langfuse...\")\n",
    "        trace_id = langfuse_evaluation_tracing(\n",
    "            evaluation_results=evaluation_results,\n",
    "            agent_id=AGENT_ID,\n",
    "            agent_alias=AGENT_ALIAS_ID,\n",
    "            evaluator_model=EVALUATOR_MODEL,\n",
    "            aws_region=AWS_REGION,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Langfuse tracing completed successfully!\")\n",
    "        print(f\"🔗 Trace ID: {trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Langfuse tracing failed: {str(e)}\")\n",
    "        print(\"📝 Evaluation results are still saved to CSV file.\")\n",
    "        print(\"💡 Check your Langfuse configuration in config.json\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
