{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases Agent Evaluator (Amazon Bedrock Agents)\n",
    "\n",
    "This notebook provides a simplified agent evaluation framework that evaluates only the specific questions from YAML test files with exactly one turn per question.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- **Direct question evaluation from YAML file**\n",
    "- **One turn per question evaluation**\n",
    "- **YAML format support for better readability**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test YAML file (e.g., `test_questions.yml`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip installboto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich langfuse==3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import yaml\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Langfuse SDK import for direct tracing\n",
    "from langfuse import Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent and LangFuse Configuration\n",
    "\n",
    "Load the configuration from config.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration loaded successfully!\n",
      "🏷️  Project: Financial-markets-super-agent\n",
      "🌍 Environment: qa\n",
      "🤖 Agent ID: CARG5UXPD9\n",
      "🤖 Agent Alias ID: 0RV9TBGQC4\n",
      "🔗 Langfuse URL: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"📋 Configuration loaded successfully!\")\n",
    "print(f\"🏷️  Project: {config['langfuse']['project_name']}\")\n",
    "print(f\"🌍 Environment: {config['langfuse']['environment']}\")\n",
    "print(f\"🤖 Agent ID: {config['agent']['agentId']}\")\n",
    "print(f\"🤖 Agent Alias ID: {config['agent']['agentAliasId']}\")\n",
    "print(f\"🔗 Langfuse URL: {config['langfuse']['langfuse_api_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Langfuse Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Langfuse endpoint: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com/api/public/otel/v1/traces\n",
      "🏷️  Project: Financial-markets-super-agent, Environment: qa\n"
     ]
    }
   ],
   "source": [
    "# Set up Langfuse configuration\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"LANGFUSE_TRACING_ENVIRONMENT\"] = 'qa'\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"📊 Langfuse endpoint: {os.environ['OTEL_EXPORTER_OTLP_ENDPOINT']}\")\n",
    "print(f\"🏷️  Project: {project_name}, Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for conversation handling and Bedrock agent communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles communication with different Bedrock model providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agent Evaluator Class \n",
    "\n",
    "The main evaluator class that evaluates only the specific questions from JSON file with one turn per question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator - FIXED to evaluate only JSON questions with one turn each.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\"):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags with improved error handling.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            try:\n",
    "                # Try exact match first\n",
    "                pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "                match = re.search(pattern, xml_data, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted = match.group(1).strip()\n",
    "                    content.append(extracted if extracted else None)\n",
    "                else:\n",
    "                    # Try case-insensitive match\n",
    "                    pattern = rf\"<{e.lower()}>(.*?)</{e.lower()}>\"\n",
    "                    match = re.search(pattern, xml_data.lower(), re.DOTALL)\n",
    "                    if match:\n",
    "                        # Find the original case version\n",
    "                        start_tag = f\"<{e.lower()}>\"\n",
    "                        end_tag = f\"</{e.lower()}>\"\n",
    "                        start_idx = xml_data.lower().find(start_tag)\n",
    "                        end_idx = xml_data.lower().find(end_tag)\n",
    "                        if start_idx != -1 and end_idx != -1:\n",
    "                            extracted = xml_data[start_idx + len(start_tag):end_idx].strip()\n",
    "                            content.append(extracted if extracted else None)\n",
    "                        else:\n",
    "                            content.append(None)\n",
    "                    else:\n",
    "                        content.append(None)\n",
    "            except Exception as ex:\n",
    "                print(f\"Warning: Error extracting {e} from XML: {ex}\")\n",
    "                content.append(None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_evaluation(self, expected_result: str, question: str, agent_response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of a single question-answer pair against expected result.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating an agent's response to a user question.\n",
    "\n",
    "Your job is to analyze the user question, agent response, and expected result to determine if the agent's response meets the expected criteria.\n",
    "\n",
    "You will classify the response into the following categories:\n",
    "\n",
    "- A: The agent's response meets or exceeds the expected result criteria.\n",
    "- B: The agent's response does not meet the expected result criteria.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter within <category> tags. Evaluation Output must be in Spanish\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Here is the evaluation scenario:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<agent_response>\n",
    "{agent_response}\n",
    "</agent_response>\n",
    "\n",
    "<expected_result>\n",
    "{expected_result}\n",
    "</expected_result>\n",
    "\n",
    "Evaluate whether the agent's response meets the expected result criteria.\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with questions from JSON file - one turn per question.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        # Process each question as a separate turn\n",
    "        for i, (question, expected_result) in enumerate(zip(questions, expected_results)):\n",
    "            print(f\"\\nTurn {i + 1}\")\n",
    "            print(f\"USER: {question}\")\n",
    "            \n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(question)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(question, agent_response)\n",
    "            \n",
    "            # Evaluate this specific question-answer pair\n",
    "            eval_category, reasoning = self._generate_evaluation(expected_result, question, agent_response)\n",
    "            \n",
    "            question_passed = eval_category == \"A\"\n",
    "            question_result = {\n",
    "                \"question_number\": i + 1,\n",
    "                \"question\": question,\n",
    "                \"expected_result\": expected_result,\n",
    "                \"agent_response\": agent_response,\n",
    "                \"passed\": question_passed,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "            all_results.append(question_result)\n",
    "            \n",
    "            print(f\"Question {i + 1} Status: {'PASSED' if question_passed else 'FAILED'}\")\n",
    "        \n",
    "        # Overall test passes if all questions pass\n",
    "        overall_passed = all(result[\"passed\"] for result in all_results)\n",
    "        \n",
    "        if overall_passed:\n",
    "            overall_result = \"All questions in the test passed - expected results observed.\"\n",
    "        else:\n",
    "            failed_questions = [str(r[\"question_number\"]) for r in all_results if not r[\"passed\"]]\n",
    "            overall_result = f\"Test failed - questions {', '.join(failed_questions)} did not meet expected results.\"\n",
    "        \n",
    "        # Combine all reasoning\n",
    "        combined_reasoning = \" | \".join([f\"Q{r['question_number']}: {r['reasoning']}\" for r in all_results if r['reasoning']])\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": overall_passed,\n",
    "            \"result\": overall_result,\n",
    "            \"reasoning\": combined_reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns,\n",
    "            \"question_results\": all_results\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the YAML file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from YAML: {tests_file_path}\")\n",
    "        \n",
    "        # Load YAML file instead of JSON\n",
    "        with open(tests_file_path, 'r', encoding='utf-8') as f:\n",
    "            tests_data = yaml.safe_load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export Function\n",
    "\n",
    "This function exports evaluation results to a CSV file with detailed information for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results_to_csv(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                                   output_filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to a CSV file with detailed question-by-question information.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        output_filename: Optional custom filename. If None, generates timestamp-based name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"agent_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure .csv extension\n",
    "    if not output_filename.endswith('.csv'):\n",
    "        output_filename += '.csv'\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define evaluation results directory\n",
    "    results_dir = os.path.join(current_dir, \"evaluation_results\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define complete output path\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = [\n",
    "        'AGENT_ID',\n",
    "        'AGENT_ALIAS', \n",
    "        'TEST_NAME',\n",
    "        'QUESTION_NUMBER',\n",
    "        'QUESTION',\n",
    "        'EXPECTED_RESULT',\n",
    "        'AGENT_RESPONSE',\n",
    "        'QUESTION_PASSED',\n",
    "        'TEST_PASSED',\n",
    "        'REASONING'\n",
    "    ]\n",
    "    \n",
    "    # Write CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Process each test result\n",
    "        for result in evaluation_results['results']:\n",
    "            test_name = result['test_name']\n",
    "            test_passed = result['passed']\n",
    "            \n",
    "            # Process each question in the test\n",
    "            for question_result in result.get('question_results', []):\n",
    "                row = [\n",
    "                    agent_id,\n",
    "                    agent_alias,\n",
    "                    test_name,\n",
    "                    question_result['question_number'],\n",
    "                    question_result['question'],\n",
    "                    question_result['expected_result'],\n",
    "                    question_result['agent_response'],\n",
    "                    question_result['passed'],\n",
    "                    test_passed,\n",
    "                    question_result['reasoning'] or 'No reasoning provided'\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\n📊 Evaluation results saved to: {output_path}\")\n",
    "    print(f\"📈 Total rows written: {sum(len(result.get('question_results', [])) for result in evaluation_results['results'])}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Evaluation Tracing Function\n",
    "\n",
    "This function traces evaluation results to Langfuse v3 with a single trace containing agent metadata and individual spans for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langfuse_evaluation_tracing(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                               evaluator_model: str, aws_region: str, config: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Trace evaluation results to Langfuse v3 with a single trace containing agent metadata\n",
    "    and individual spans for each test question.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        evaluator_model: The model used for evaluation\n",
    "        aws_region: AWS region\n",
    "        config: Configuration dictionary with Langfuse credentials\n",
    "    \n",
    "    Returns:\n",
    "        str: Trace ID of the created trace\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize Langfuse client using config\n",
    "        langfuse_client = Langfuse(\n",
    "            secret_key=config['langfuse']['langfuse_secret_key'],\n",
    "            public_key=config['langfuse']['langfuse_public_key'],\n",
    "            host=config['langfuse']['langfuse_api_url']        )\n",
    "        \n",
    "        print(\"🔗 Langfuse client initialized successfully\")\n",
    "        \n",
    "        # Initialize timestamp for metadata\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tests = evaluation_results['total_tests']\n",
    "        passed_tests = evaluation_results['passed_tests']\n",
    "        pass_rate = evaluation_results['pass_rate']\n",
    "        \n",
    "        # Create main trace with agent metadata and overall results (Langfuse v3 correct API)\n",
    "        main_trace = langfuse_client.start_span(\n",
    "            name=f\"Bedrock Agent Evaluation - {agent_id}\",\n",
    "            input={\n",
    "                \"agent_metadata\": {\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"agent_alias\": agent_alias,\n",
    "                    \"aws_region\": aws_region,\n",
    "                    \"evaluator_model\": evaluator_model,\n",
    "                    \"evaluation_timestamp\": timestamp.isoformat(),\n",
    "                    \"project_name\": config['langfuse']['project_name']                }\n",
    "            },\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            },\n",
    "            metadata={\n",
    "                \"evaluation_framework\": \"CAT Agent Evaluator\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"langfuse_version\": \"v3\",\n",
    "                \"project\": config['langfuse']['project_name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get the actual trace ID from the span\n",
    "        trace_id = main_trace.trace_id\n",
    "        print(f\"📊 Main trace created with ID: {trace_id}\")\n",
    "\n",
    "        # Add tags to the trace using update_trace()\n",
    "        main_trace.update_trace(tags=[config['langfuse']['agentTag'], f\"BR-AgentID-{agent_id}\", f\"BR-AgentAlias-{agent_alias}\"])\n",
    "        \n",
    "        # Process each test result and create spans\n",
    "        span_count = 0\n",
    "        for test_result in evaluation_results['results']:\n",
    "            test_name = test_result['test_name']\n",
    "            test_passed = test_result['passed']\n",
    "            \n",
    "            # Create a span for each question in the test\n",
    "            for question_result in test_result.get('question_results', []):\n",
    "                question_id = f\"{test_name}_q{question_result['question_number']}\"\n",
    "                \n",
    "                # Create span for individual question (Langfuse v3 correct API)\n",
    "                question_span = main_trace.start_span(\n",
    "                    name=f\"Question: {test_name} - Q{question_result['question_number']}\",\n",
    "                    input={\n",
    "                        \"test_name\": test_name,\n",
    "                        \"question_id\": question_id,\n",
    "                        \"question\": question_result['question'],\n",
    "                        \"expected_result\": question_result['expected_result']\n",
    "                    },\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"question_number\": question_result['question_number'],\n",
    "                        \"test_passed\": test_passed,\n",
    "                        \"evaluation_category\": \"A\" if question_result['passed'] else \"B\",\n",
    "                        \"test_name\": test_name\n",
    "                    },\n",
    "                    level=\"DEFAULT\"\n",
    "                )\n",
    "                \n",
    "                # Update and end the question span\n",
    "                question_span.update(\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    }\n",
    "                )\n",
    "                question_span.end()\n",
    "                \n",
    "                # Add score to the span (Langfuse v3 correct API)\n",
    "                langfuse_client.create_score(\n",
    "                    trace_id=trace_id,\n",
    "                    observation_id=question_span.id,\n",
    "                    name=\"question_evaluation\",\n",
    "                    value=1.0 if question_result['passed'] else 0.0,\n",
    "                    comment=question_result['reasoning'] or \"No reasoning provided\"\n",
    "                )\n",
    "                \n",
    "                span_count += 1\n",
    "        \n",
    "        # Update the main span\n",
    "        main_trace.update(\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update and end the main trace\n",
    "        main_trace.update_trace(\n",
    "                    input={\n",
    "                        \"EVALUATION_FILE\": config['agent']['evaluation_file']\n",
    "                        },\n",
    "                    output={\n",
    "                        \"PASSED\": passed_tests == total_tests\n",
    "                        }\n",
    "                )\n",
    "        \n",
    "        main_trace.end()\n",
    "        \n",
    "        # Add overall score to the trace (Langfuse v3 correct API)\n",
    "        overall_score = float(passed_tests) / float(total_tests) if total_tests > 0 else 0.0\n",
    "        langfuse_client.create_score(\n",
    "            trace_id=trace_id,\n",
    "            observation_id=main_trace.id,\n",
    "            name=\"overall_evaluation\",\n",
    "            value=overall_score,\n",
    "            comment=f\"Overall pass rate: {pass_rate}. {passed_tests}/{total_tests} tests passed.\"\n",
    "        )\n",
    "        \n",
    "        # Flush to ensure data is sent\n",
    "        langfuse_client.flush()\n",
    "        \n",
    "        print(f\"\\n🚀 Evaluation results traced to Langfuse successfully!\")\n",
    "        print(f\"📊 Trace ID: {trace_id}\")\n",
    "        print(f\"📈 Total spans created: {span_count}\")\n",
    "        print(f\"🎯 Overall score: {overall_score:.2f}\")\n",
    "        print(f\"🔗 Project: {config['langfuse']['project_name']} | Environment: {config['langfuse']['environment']}\")\n",
    "        \n",
    "        return trace_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error tracing to Langfuse: {str(e)}\")\n",
    "        print(\"💡 Make sure your Langfuse configuration is correct in config.json\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution \n",
    "\n",
    "Configure the evaluation parameters and run the evaluation with the fixed logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation with YAML...\n",
      "Evaluator Model: us.amazon.nova-premier-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Test File: yfin_test_questions.yml (YAML format)\n",
      "Loading tests from YAML: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/yfin_test_questions.yml\n",
      "\n",
      "=== Evaluating Test: analisis_criptomonedas ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Puedes ayudarme a entender cómo está desempeñándose el mercado cripto hoy?\n",
      "AGENT: Basándome en los datos actuales del mercado de criptomonedas, puedo ofrecerte un resumen:\n",
      "\n",
      "🔹 Principales características:\n",
      "- Bitcoin (BTC) está alrededor de $109,865\n",
      "- Ethereum (ETH) se cotiza cerca de $4,272\n",
      "- Algunas monedas estables (como USDT) se mantienen cerca de $1\n",
      "\n",
      "El mercado muestra variedad de precios, desde monedas de centavos hasta aquellas con valores de miles de dólares. Las criptomonedas principales mantienen una cotización estable.\n",
      "\n",
      "¿Te gustaría que profundice en algún aspecto específico del mercado cripto?\n",
      "Question 1 Status: PASSED\n",
      "\n",
      "Turn 2\n",
      "USER: Basándo en esa información, ¿cuáles son las 3 mejores criptomonedas que tuvieron el mejor rendimiento hoy?\n",
      "AGENT: Basándome en los datos previos, las 3 criptomonedas con mejor desempeño son:\n",
      "\n",
      "1. Bitcoin (BTC): $109,865 - La criptomoneda líder del mercado\n",
      "2. Ethereum (ETH): $4,272 - Segunda criptomoneda más importante\n",
      "3. Una moneda con precio de $5,201 (posiblemente otra altcoin importante)\n",
      "\n",
      "Sin embargo, para darte una recomendación más precisa sobre rendimiento, necesitaría información adicional sobre variaciones porcentuales y volumen de trading, que no están incluidos en los datos actuales.\n",
      "\n",
      "¿Te gustaría que busque más detalles específicos sobre su rendimiento?\n",
      "Question 2 Status: FAILED\n",
      "\n",
      "Turn 3\n",
      "USER: Amplia información sobre la mejor criptomoneda de tu anterior analisis\n",
      "AGENT: 🔹 Bitcoin (BTC) - Análisis Detallado:\n",
      "\n",
      "Características Principales:\n",
      "- Precio Actual: $109,865\n",
      "- Posición: Líder indiscutible del mercado de criptomonedas\n",
      "- Capitalización: La más alta en el mercado cripto\n",
      "\n",
      "Aspectos Destacados:\n",
      "- Primera criptomoneda creada en el mundo\n",
      "- Considerada un activo de reserva digital\n",
      "- Ampliamente aceptada como inversión y medio de intercambio\n",
      "- Tecnología blockchain que garantiza transacciones seguras\n",
      "\n",
      "Puntos Importantes:\n",
      "- Limitada a 21 millones de unidades totales\n",
      "- Altamente volátil pero con tendencia de valor a largo plazo\n",
      "- Respaldada por una comunidad global de inversores y desarrolladores\n",
      "\n",
      "Recomendación: Siempre realizar una investigación personal y consultar con asesores financieros antes de invertir.\n",
      "\n",
      "¿Deseas que profundice en algún aspecto específico de Bitcoin?\n",
      "Question 3 Status: PASSED\n",
      "\n",
      "=== Evaluating Test: noticias_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Cuáles son las noticias más importantes del mercado de valores de hoy?\n",
      "AGENT: Entiendo que estás interesado en las noticias más importantes del mercado de valores de hoy. Para proporcionarte información precisa y relevante, necesito saber sobre qué acción o empresa en particular te gustaría obtener noticias. \n",
      "\n",
      "¿Hay alguna empresa o sector específico del mercado de valores sobre el que te gustaría conocer las últimas noticias? Por ejemplo, ¿te interesa saber sobre empresas tecnológicas como Apple (AAPL), gigantes del comercio electrónico como Amazon (AMZN), o quizás sobre índices generales del mercado?\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "=== Evaluating Test: informacion_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¿Cuál es el precio actual de las acciones de Tesla y su variación porcentual del día?\n",
      "AGENT: Lo siento, en este momento estoy experimentando dificultades técnicas para obtener la información precisa sobre el precio de las acciones de Tesla. Los sistemas de información financiera parecen estar presentando un error momentáneo.\n",
      "\n",
      "Lo que puedo recomendarte es:\n",
      "1. Verificar el precio actual en una plataforma financiera en línea\n",
      "2. Consultar tu aplicación de inversiones preferida\n",
      "3. Revisar sitios web especializados como Yahoo Finance o Google Finance\n",
      "\n",
      "Si deseas, puedo intentar ayudarte de nuevo más tarde para obtener los datos exactos de Tesla. ¿Te gustaría que lo intentemos en unos momentos?\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "Turn 2\n",
      "USER: ¿Puedes mostrarme el rendimiento semanal de NVIDIA comparado con el índice S&P 500?\n",
      "AGENT: Lo siento, actualmente no puedo proporcionarte los datos exactos sobre el rendimiento semanal de NVIDIA en comparación con el S&P 500 debido a limitaciones técnicas en el acceso a datos financieros actualizados.\n",
      "\n",
      "Lo que puedo compartir es la siguiente información:\n",
      "\n",
      "Para el S&P 500:\n",
      "- Nivel actual: 6,490.52 puntos\n",
      "- Cambio diario: +42.26 puntos (+0.66%)\n",
      "\n",
      "Para NVIDIA (NVDA):\n",
      "- Información general: Empresa líder en el sector de semiconductores\n",
      "- Especializada en chips para inteligencia artificial y computación gráfica\n",
      "- Ha mostrado un fuerte desempeño reciente en el mercado tecnológico\n",
      "\n",
      "Para obtener la información específica sobre el rendimiento semanal comparativo que estás buscando, te recomendaría consultar:\n",
      "1. Plataformas financieras como Yahoo Finance, Google Finance o Bloomberg\n",
      "2. Servicios de corretaje en línea\n",
      "3. Aplicaciones financieras especializadas\n",
      "\n",
      "¿Te gustaría que te ayude con otro tipo de análisis o información financiera que pueda proporcionarte en este momento?\n",
      "Question 2 Status: FAILED\n",
      "\n",
      "============================================================\n",
      "YAML EVALUATION SUMMARY\n",
      "============================================================\n",
      "Pass Rate: 0.0%\n",
      "Tests Passed: 0/3\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "Test: analisis_criptomonedas\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 2 did not meet expected results.\n",
      "Reasoning: Q1: La respuesta del agente proporciona información actualizada sobre los precios de criptomonedas importantes como Bitcoin, Ethereum y monedas estables, lo que da una visión general del estado actual del mercado. Además, menciona la diversidad de precios en el mercado y la estabilidad de las criptomonedas principales, lo cual contribuye a entender el desempeño general. Finalmente, ofrece profundizar en aspectos específicos, mostrando disposición para dar más detalles. \n",
      "\n",
      "El resultado esperado era un análisis del mercado cripto hoy, y aunque la respuesta no incluye análisis técnico detallado o tendencias específicas, sí ofrece una descripción básica del estado del mercado que podría considerarse suficiente para una consulta general. Sin embargo, falta información sobre tendencias diarias, volúmenes de negociación o factores influyentes, que podrían enriquecer el análisis. Aún así, cumple con proporcionar una situación general. | Q2: La pregunta del usuario solicita específicamente las 3 criptomonedas con mejor rendimiento del día, lo que implica cambios porcentuales diarios. La respuesta del agente menciona tres criptomonedas pero no proporciona datos de rendimiento diario ni variaciones porcentuales, que son fundamentales para evaluar el \"mejor rendimiento hoy\". Aunque menciona precios actuales, esto no refleja el rendimiento diario. Además, la tercera opción es indeterminada (\"posiblemente otra altcoin\"), lo que muestra falta de precisión. El agente reconoce la falta de datos sobre variaciones porcentuales pero no resuelve la pregunta original. Por tanto, no cumple con el criterio esperado de identificar las criptomonedas con mejor rendimiento diario. | Q3: La pregunta del usuario solicita información ampliada sobre \"la mejor criptomoneda de tu anterior análisis\". El agente responde con un análisis detallado de Bitcoin, mencionando su precio, posición, capitalización, características históricas, tecnológicas y de adopción. Aunque no especifica si Bitcoin era la criptomoneda destacada previamente, proporciona información sustancial que podría satisfacer la solicitud si Bitcoin fuera la criptomoneda analizada. La respuesta incluye datos específicos y recomendaciones, cumpliendo con la necesidad de información detallada. Sin embargo, falta confirmación explícita de que Bitcoin era el tema del análisis anterior, pero la profundidad de la información probablemente compensa esta ausencia.\n",
      "Questions Evaluated: 3\n",
      "\n",
      "Question Details:\n",
      "  Q1: ✅ PASSED\n",
      "    Question: ¿Puedes ayudarme a entender cómo está desempeñándose el mercado cripto hoy?...\n",
      "    Reasoning: La respuesta del agente proporciona información actualizada sobre los precios de criptomonedas importantes como Bitcoin, Ethereum y monedas estables, lo que da una visión general del estado actual del mercado. Además, menciona la diversidad de precios en el mercado y la estabilidad de las criptomonedas principales, lo cual contribuye a entender el desempeño general. Finalmente, ofrece profundizar en aspectos específicos, mostrando disposición para dar más detalles. \n",
      "\n",
      "El resultado esperado era un análisis del mercado cripto hoy, y aunque la respuesta no incluye análisis técnico detallado o tendencias específicas, sí ofrece una descripción básica del estado del mercado que podría considerarse suficiente para una consulta general. Sin embargo, falta información sobre tendencias diarias, volúmenes de negociación o factores influyentes, que podrían enriquecer el análisis. Aún así, cumple con proporcionar una situación general.\n",
      "  Q2: ❌ FAILED\n",
      "    Question: Basándo en esa información, ¿cuáles son las 3 mejores criptomonedas que tuvieron el mejor rendimient...\n",
      "    Reasoning: La pregunta del usuario solicita específicamente las 3 criptomonedas con mejor rendimiento del día, lo que implica cambios porcentuales diarios. La respuesta del agente menciona tres criptomonedas pero no proporciona datos de rendimiento diario ni variaciones porcentuales, que son fundamentales para evaluar el \"mejor rendimiento hoy\". Aunque menciona precios actuales, esto no refleja el rendimiento diario. Además, la tercera opción es indeterminada (\"posiblemente otra altcoin\"), lo que muestra falta de precisión. El agente reconoce la falta de datos sobre variaciones porcentuales pero no resuelve la pregunta original. Por tanto, no cumple con el criterio esperado de identificar las criptomonedas con mejor rendimiento diario.\n",
      "  Q3: ✅ PASSED\n",
      "    Question: Amplia información sobre la mejor criptomoneda de tu anterior analisis...\n",
      "    Reasoning: La pregunta del usuario solicita información ampliada sobre \"la mejor criptomoneda de tu anterior análisis\". El agente responde con un análisis detallado de Bitcoin, mencionando su precio, posición, capitalización, características históricas, tecnológicas y de adopción. Aunque no especifica si Bitcoin era la criptomoneda destacada previamente, proporciona información sustancial que podría satisfacer la solicitud si Bitcoin fuera la criptomoneda analizada. La respuesta incluye datos específicos y recomendaciones, cumpliendo con la necesidad de información detallada. Sin embargo, falta confirmación explícita de que Bitcoin era el tema del análisis anterior, pero la profundidad de la información probablemente compensa esta ausencia.\n",
      "----------------------------------------\n",
      "\n",
      "Test: noticias_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1 did not meet expected results.\n",
      "Reasoning: Q1: La pregunta del usuario solicita directamente las noticias más importantes del mercado de valores del día. El agente, en lugar de proporcionar información específica sobre eventos actuales del mercado, pide aclaraciones sobre acciones o sectores específicos. Aunque la solicitud de clarificación podría ser útil en otros contextos, aquí no cumple con la expectativa explícita del usuario de recibir noticias actuales del mercado general. El agente no menciona ninguna noticia real ni proporciona datos del día, lo que hace que la respuesta no satisfaga el resultado esperado de ofrecer un resumen de noticias relevantes.\n",
      "Questions Evaluated: 1\n",
      "\n",
      "Question Details:\n",
      "  Q1: ❌ FAILED\n",
      "    Question: ¿Cuáles son las noticias más importantes del mercado de valores de hoy?...\n",
      "    Reasoning: La pregunta del usuario solicita directamente las noticias más importantes del mercado de valores del día. El agente, en lugar de proporcionar información específica sobre eventos actuales del mercado, pide aclaraciones sobre acciones o sectores específicos. Aunque la solicitud de clarificación podría ser útil en otros contextos, aquí no cumple con la expectativa explícita del usuario de recibir noticias actuales del mercado general. El agente no menciona ninguna noticia real ni proporciona datos del día, lo que hace que la respuesta no satisfaga el resultado esperado de ofrecer un resumen de noticias relevantes.\n",
      "----------------------------------------\n",
      "\n",
      "Test: informacion_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1, 2 did not meet expected results.\n",
      "Reasoning: Q1: La pregunta del usuario solicita específicamente dos datos concretos: el precio actual de las acciones de Tesla y su variación porcentual diaria. El resultado esperado requiere que el agente proporcione estos datos numéricos actualizados.\n",
      "\n",
      "La respuesta del agente explica que tiene dificultades técnicas para obtener la información y sugiere tres alternativas para que el usuario encuentre la respuesta por su cuenta. Aunque ofrece recomendaciones útiles y muestra empatía, no proporciona el precio actual ni la variación porcentual requerida. \n",
      "\n",
      "El agente no cumple con la entrega directa de los datos solicitados, que era el objetivo principal de la consulta. Aunque la explicación sobre problemas técnicos es válida, no satisface completamente la necesidad inmediata del usuario. La respuesta no incluye los valores numéricos necesarios ni intenta ofrecer una solución alternativa inmediata (como datos aproximados si estuvieran disponibles).\n",
      "\n",
      "Por lo tanto, la respuesta no cumple con los criterios esperados de proporcionar información financiera específica y actualizada. | Q2: La pregunta del usuario solicita específicamente el rendimiento semanal comparativo entre NVIDIA y el S&P 500. El resultado esperado requiere datos concretos de rendimiento porcentual y posiblemente gráficos. \n",
      "\n",
      "La respuesta del agente explica que no puede proporcionar datos exactos debido a limitaciones técnicas, ofrece información general sobre ambas entidades y sugiere fuentes alternativas. Aunque proporciona contexto útil y recomendaciones válidas, no cumple con el requisito principal de mostrar el rendimiento semanal comparativo. \n",
      "\n",
      "El agente no menciona porcentajes semanales ni intenta aproximar la comparación, lo que es esencial según el resultado esperado. Aunque la respuesta es educada y ofrece alternativas, no satisface la solicitud directa del usuario. Por tanto, no cumple con los criterios esperados.\n",
      "Questions Evaluated: 2\n",
      "\n",
      "Question Details:\n",
      "  Q1: ❌ FAILED\n",
      "    Question: ¿Cuál es el precio actual de las acciones de Tesla y su variación porcentual del día?...\n",
      "    Reasoning: La pregunta del usuario solicita específicamente dos datos concretos: el precio actual de las acciones de Tesla y su variación porcentual diaria. El resultado esperado requiere que el agente proporcione estos datos numéricos actualizados.\n",
      "\n",
      "La respuesta del agente explica que tiene dificultades técnicas para obtener la información y sugiere tres alternativas para que el usuario encuentre la respuesta por su cuenta. Aunque ofrece recomendaciones útiles y muestra empatía, no proporciona el precio actual ni la variación porcentual requerida. \n",
      "\n",
      "El agente no cumple con la entrega directa de los datos solicitados, que era el objetivo principal de la consulta. Aunque la explicación sobre problemas técnicos es válida, no satisface completamente la necesidad inmediata del usuario. La respuesta no incluye los valores numéricos necesarios ni intenta ofrecer una solución alternativa inmediata (como datos aproximados si estuvieran disponibles).\n",
      "\n",
      "Por lo tanto, la respuesta no cumple con los criterios esperados de proporcionar información financiera específica y actualizada.\n",
      "  Q2: ❌ FAILED\n",
      "    Question: ¿Puedes mostrarme el rendimiento semanal de NVIDIA comparado con el índice S&P 500?...\n",
      "    Reasoning: La pregunta del usuario solicita específicamente el rendimiento semanal comparativo entre NVIDIA y el S&P 500. El resultado esperado requiere datos concretos de rendimiento porcentual y posiblemente gráficos. \n",
      "\n",
      "La respuesta del agente explica que no puede proporcionar datos exactos debido a limitaciones técnicas, ofrece información general sobre ambas entidades y sugiere fuentes alternativas. Aunque proporciona contexto útil y recomendaciones válidas, no cumple con el requisito principal de mostrar el rendimiento semanal comparativo. \n",
      "\n",
      "El agente no menciona porcentajes semanales ni intenta aproximar la comparación, lo que es esencial según el resultado esperado. Aunque la respuesta es educada y ofrece alternativas, no satisface la solicitud directa del usuario. Por tanto, no cumple con los criterios esperados.\n",
      "----------------------------------------\n",
      "\n",
      "📊 Evaluation results saved to: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/evaluation_results/agent_evaluation_results_20250904_194436.csv\n",
      "📈 Total rows written: 6\n",
      "\n",
      "🔄 Tracing evaluation results to Langfuse...\n",
      "🔗 Langfuse client initialized successfully\n",
      "📊 Main trace created with ID: 871051eec2dd790a919530aa35ac01c6\n",
      "\n",
      "🚀 Evaluation results traced to Langfuse successfully!\n",
      "📊 Trace ID: 871051eec2dd790a919530aa35ac01c6\n",
      "📈 Total spans created: 6\n",
      "🎯 Overall score: 0.00\n",
      "🔗 Project: Financial-markets-super-agent | Environment: qa\n",
      "✅ Langfuse tracing completed successfully!\n",
      "🔗 Trace ID: 871051eec2dd790a919530aa35ac01c6\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation - one turn per question only.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"us.amazon.nova-premier-v1:0\", \"us.amazon.nova-pro-v1:0\", \"us.amazon.nova-lite-v1:0\", \"us.amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "\n",
    "    EVALUATOR_MODEL = config['agent']['evaluator_model']  # Evaluation model cross-region endpoint \n",
    "    AGENT_ID = config['agent']['agentId']  #  your actual agent ID\n",
    "    AGENT_ALIAS_ID = config['agent']['agentAliasId']  # your actual alias ID\n",
    "    AWS_REGION = config['agent']['region']  # agent's region\n",
    "    EVAL_FILE = config['agent']['evaluation_file']  # YAML file with evaluation tests and questions\n",
    "\n",
    "    # Initialize evaluator (removed max_turns parameter - not needed anymore)\n",
    "    evaluator = CATAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation with YAML...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "    print(f\"Test File: {EVAL_FILE} (YAML format)\")\n",
    "\n",
    "    evaluation_results = evaluator.run_evaluation(EVAL_FILE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"YAML EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Questions Evaluated: {result['turns']}\")\n",
    "        \n",
    "        # Show individual question results\n",
    "        if 'question_results' in result:\n",
    "            print(\"\\nQuestion Details:\")\n",
    "            for q_result in result['question_results']:\n",
    "                status = \"✅ PASSED\" if q_result['passed'] else \"❌ FAILED\"\n",
    "                print(f\"  Q{q_result['question_number']}: {status}\")\n",
    "                print(f\"    Question: {q_result['question'][:100]}...\")\n",
    "                print(f\"    Reasoning: {q_result['reasoning']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = save_evaluation_results_to_csv(\n",
    "        evaluation_results=evaluation_results,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias=AGENT_ALIAS_ID\n",
    "    )\n",
    "    \n",
    "    # Trace results to Langfuse\n",
    "    try:\n",
    "        print(\"\\n🔄 Tracing evaluation results to Langfuse...\")\n",
    "        trace_id = langfuse_evaluation_tracing(\n",
    "            evaluation_results=evaluation_results,\n",
    "            agent_id=AGENT_ID,\n",
    "            agent_alias=AGENT_ALIAS_ID,\n",
    "            evaluator_model=EVALUATOR_MODEL,\n",
    "            aws_region=AWS_REGION,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Langfuse tracing completed successfully!\")\n",
    "        print(f\"🔗 Trace ID: {trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Langfuse tracing failed: {str(e)}\")\n",
    "        print(\"📝 Evaluation results are still saved to CSV file.\")\n",
    "        print(\"💡 Check your Langfuse configuration in config.json\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
