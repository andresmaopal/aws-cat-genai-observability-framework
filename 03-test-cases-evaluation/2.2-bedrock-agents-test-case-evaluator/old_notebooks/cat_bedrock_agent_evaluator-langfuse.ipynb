{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT Agent Evaluator \n",
    "\n",
    "This notebook provides a simplified agent evaluation framework that evaluates only the specific questions from the tests_structure.json file with exactly one turn per question.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- **Direct question evaluation from JSON file**\n",
    "- **One turn per question evaluation**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test JSON file (e.g., `tests_structure.json`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (3.1.6)\n",
      "Requirement already satisfied: jsonpath-ng in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: markdown-it-py in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: langfuse==3.1.1 in /opt/conda/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: packaging<25.0,>=23.2 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (2.32.3)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /opt/conda/lib/python3.12/site-packages (from langfuse==3.1.1) (1.17.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2) (3.0.2)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.12/site-packages (from jsonpath-ng) (3.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.1.1) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.1.1) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.1.1) (6.10.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (1.36.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp<2.0.0,>=1.33.1->langfuse==3.1.1) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.1.1) (0.57b0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.1.1) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.1.1) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx<1.0,>=0.15.4->langfuse==3.1.1) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich langfuse==3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Langfuse SDK import for direct tracing\n",
    "from langfuse import Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent and LangFuse Configuration\n",
    "\n",
    "Load the configuration from config.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration loaded successfully!\n",
      "üè∑Ô∏è  Project: Financial-markets-super-agent\n",
      "üåç Environment: qa\n",
      "ü§ñ Agent ID: CARG5UXPD9\n",
      "ü§ñ Agent Alias ID: 0RV9TBGQC4\n",
      "üîó Langfuse URL: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"üìã Configuration loaded successfully!\")\n",
    "print(f\"üè∑Ô∏è  Project: {config['langfuse']['project_name']}\")\n",
    "print(f\"üåç Environment: {config['langfuse']['environment']}\")\n",
    "print(f\"ü§ñ Agent ID: {config['agent']['agentId']}\")\n",
    "print(f\"ü§ñ Agent Alias ID: {config['agent']['agentAliasId']}\")\n",
    "print(f\"üîó Langfuse URL: {config['langfuse']['langfuse_api_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Langfuse Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Langfuse endpoint: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com/api/public/otel/v1/traces\n",
      "üè∑Ô∏è  Project: Financial-markets-super-agent, Environment: qa\n"
     ]
    }
   ],
   "source": [
    "# Set up Langfuse configuration\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"LANGFUSE_TRACING_ENVIRONMENT\"] = 'qa'\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"üìä Langfuse endpoint: {os.environ['OTEL_EXPORTER_OTLP_ENDPOINT']}\")\n",
    "print(f\"üè∑Ô∏è  Project: {project_name}, Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for conversation handling and Bedrock agent communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles communication with different Bedrock model providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agent Evaluator Class \n",
    "\n",
    "The main evaluator class that evaluates only the specific questions from JSON file with one turn per question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator - FIXED to evaluate only JSON questions with one turn each.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\"):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags with improved error handling.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            try:\n",
    "                # Try exact match first\n",
    "                pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "                match = re.search(pattern, xml_data, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted = match.group(1).strip()\n",
    "                    content.append(extracted if extracted else None)\n",
    "                else:\n",
    "                    # Try case-insensitive match\n",
    "                    pattern = rf\"<{e.lower()}>(.*?)</{e.lower()}>\"\n",
    "                    match = re.search(pattern, xml_data.lower(), re.DOTALL)\n",
    "                    if match:\n",
    "                        # Find the original case version\n",
    "                        start_tag = f\"<{e.lower()}>\"\n",
    "                        end_tag = f\"</{e.lower()}>\"\n",
    "                        start_idx = xml_data.lower().find(start_tag)\n",
    "                        end_idx = xml_data.lower().find(end_tag)\n",
    "                        if start_idx != -1 and end_idx != -1:\n",
    "                            extracted = xml_data[start_idx + len(start_tag):end_idx].strip()\n",
    "                            content.append(extracted if extracted else None)\n",
    "                        else:\n",
    "                            content.append(None)\n",
    "                    else:\n",
    "                        content.append(None)\n",
    "            except Exception as ex:\n",
    "                print(f\"Warning: Error extracting {e} from XML: {ex}\")\n",
    "                content.append(None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_evaluation(self, expected_result: str, question: str, agent_response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of a single question-answer pair against expected result.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating an agent's response to a user question.\n",
    "\n",
    "Your job is to analyze the user question, agent response, and expected result to determine if the agent's response meets the expected criteria.\n",
    "\n",
    "You will classify the response into the following categories:\n",
    "\n",
    "- A: The agent's response meets or exceeds the expected result criteria.\n",
    "- B: The agent's response does not meet the expected result criteria.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter within <category> tags. Evaluation Output must be in Spanish\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Here is the evaluation scenario:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<agent_response>\n",
    "{agent_response}\n",
    "</agent_response>\n",
    "\n",
    "<expected_result>\n",
    "{expected_result}\n",
    "</expected_result>\n",
    "\n",
    "Evaluate whether the agent's response meets the expected result criteria.\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with questions from JSON file - one turn per question.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        # Process each question as a separate turn\n",
    "        for i, (question, expected_result) in enumerate(zip(questions, expected_results)):\n",
    "            print(f\"\\nTurn {i + 1}\")\n",
    "            print(f\"USER: {question}\")\n",
    "            \n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(question)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(question, agent_response)\n",
    "            \n",
    "            # Evaluate this specific question-answer pair\n",
    "            eval_category, reasoning = self._generate_evaluation(expected_result, question, agent_response)\n",
    "            \n",
    "            question_passed = eval_category == \"A\"\n",
    "            question_result = {\n",
    "                \"question_number\": i + 1,\n",
    "                \"question\": question,\n",
    "                \"expected_result\": expected_result,\n",
    "                \"agent_response\": agent_response,\n",
    "                \"passed\": question_passed,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "            all_results.append(question_result)\n",
    "            \n",
    "            print(f\"Question {i + 1} Status: {'PASSED' if question_passed else 'FAILED'}\")\n",
    "        \n",
    "        # Overall test passes if all questions pass\n",
    "        overall_passed = all(result[\"passed\"] for result in all_results)\n",
    "        \n",
    "        if overall_passed:\n",
    "            overall_result = \"All questions in the test passed - expected results observed.\"\n",
    "        else:\n",
    "            failed_questions = [str(r[\"question_number\"]) for r in all_results if not r[\"passed\"]]\n",
    "            overall_result = f\"Test failed - questions {', '.join(failed_questions)} did not meet expected results.\"\n",
    "        \n",
    "        # Combine all reasoning\n",
    "        combined_reasoning = \" | \".join([f\"Q{r['question_number']}: {r['reasoning']}\" for r in all_results if r['reasoning']])\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": overall_passed,\n",
    "            \"result\": overall_result,\n",
    "            \"reasoning\": combined_reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns,\n",
    "            \"question_results\": all_results\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the JSON file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from: {tests_file_path}\")\n",
    "        \n",
    "        with open(tests_file_path, 'r') as f:\n",
    "            tests_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export Function\n",
    "\n",
    "This function exports evaluation results to a CSV file with detailed information for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results_to_csv(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                                   output_filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to a CSV file with detailed question-by-question information.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        output_filename: Optional custom filename. If None, generates timestamp-based name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"agent_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure .csv extension\n",
    "    if not output_filename.endswith('.csv'):\n",
    "        output_filename += '.csv'\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define evaluation results directory\n",
    "    results_dir = os.path.join(current_dir, \"evaluation_results\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define complete output path\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = [\n",
    "        'AGENT_ID',\n",
    "        'AGENT_ALIAS', \n",
    "        'TEST_NAME',\n",
    "        'QUESTION_NUMBER',\n",
    "        'QUESTION',\n",
    "        'EXPECTED_RESULT',\n",
    "        'AGENT_RESPONSE',\n",
    "        'QUESTION_PASSED',\n",
    "        'TEST_PASSED',\n",
    "        'REASONING'\n",
    "    ]\n",
    "    \n",
    "    # Write CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Process each test result\n",
    "        for result in evaluation_results['results']:\n",
    "            test_name = result['test_name']\n",
    "            test_passed = result['passed']\n",
    "            \n",
    "            # Process each question in the test\n",
    "            for question_result in result.get('question_results', []):\n",
    "                row = [\n",
    "                    agent_id,\n",
    "                    agent_alias,\n",
    "                    test_name,\n",
    "                    question_result['question_number'],\n",
    "                    question_result['question'],\n",
    "                    question_result['expected_result'],\n",
    "                    question_result['agent_response'],\n",
    "                    question_result['passed'],\n",
    "                    test_passed,\n",
    "                    question_result['reasoning'] or 'No reasoning provided'\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation results saved to: {output_path}\")\n",
    "    print(f\"üìà Total rows written: {sum(len(result.get('question_results', [])) for result in evaluation_results['results'])}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Evaluation Tracing Function\n",
    "\n",
    "This function traces evaluation results to Langfuse v3 with a single trace containing agent metadata and individual spans for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langfuse_evaluation_tracing(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                               evaluator_model: str, aws_region: str, config: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Trace evaluation results to Langfuse v3 with a single trace containing agent metadata\n",
    "    and individual spans for each test question.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        evaluator_model: The model used for evaluation\n",
    "        aws_region: AWS region\n",
    "        config: Configuration dictionary with Langfuse credentials\n",
    "    \n",
    "    Returns:\n",
    "        str: Trace ID of the created trace\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize Langfuse client using config\n",
    "        langfuse_client = Langfuse(\n",
    "            secret_key=config['langfuse']['langfuse_secret_key'],\n",
    "            public_key=config['langfuse']['langfuse_public_key'],\n",
    "            host=config['langfuse']['langfuse_api_url']        )\n",
    "        \n",
    "        print(\"üîó Langfuse client initialized successfully\")\n",
    "        \n",
    "        # Initialize timestamp for metadata\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tests = evaluation_results['total_tests']\n",
    "        passed_tests = evaluation_results['passed_tests']\n",
    "        pass_rate = evaluation_results['pass_rate']\n",
    "        \n",
    "        # Create main trace with agent metadata and overall results (Langfuse v3 correct API)\n",
    "        main_trace = langfuse_client.start_span(\n",
    "            name=f\"Bedrock Agent Evaluation - {agent_id}\",\n",
    "            input={\n",
    "                \"agent_metadata\": {\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"agent_alias\": agent_alias,\n",
    "                    \"aws_region\": aws_region,\n",
    "                    \"evaluator_model\": evaluator_model,\n",
    "                    \"evaluation_timestamp\": timestamp.isoformat(),\n",
    "                    \"project_name\": config['langfuse']['project_name']                }\n",
    "            },\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            },\n",
    "            metadata={\n",
    "                \"evaluation_framework\": \"CAT Agent Evaluator\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"langfuse_version\": \"v3\",\n",
    "                \"project\": config['langfuse']['project_name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get the actual trace ID from the span\n",
    "        trace_id = main_trace.trace_id\n",
    "        print(f\"üìä Main trace created with ID: {trace_id}\")\n",
    "        \n",
    "        # Process each test result and create spans\n",
    "        span_count = 0\n",
    "        for test_result in evaluation_results['results']:\n",
    "            test_name = test_result['test_name']\n",
    "            test_passed = test_result['passed']\n",
    "            \n",
    "            # Create a span for each question in the test\n",
    "            for question_result in test_result.get('question_results', []):\n",
    "                question_id = f\"{test_name}_q{question_result['question_number']}\"\n",
    "                \n",
    "                # Create span for individual question (Langfuse v3 correct API)\n",
    "                question_span = main_trace.start_span(\n",
    "                    name=f\"Question: {test_name} - Q{question_result['question_number']}\",\n",
    "                    input={\n",
    "                        \"test_name\": test_name,\n",
    "                        \"question_id\": question_id,\n",
    "                        \"question\": question_result['question'],\n",
    "                        \"expected_result\": question_result['expected_result']\n",
    "                    },\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"question_number\": question_result['question_number'],\n",
    "                        \"test_passed\": test_passed,\n",
    "                        \"evaluation_category\": \"A\" if question_result['passed'] else \"B\",\n",
    "                        \"test_name\": test_name\n",
    "                    },\n",
    "                    level=\"DEFAULT\"\n",
    "                )\n",
    "                \n",
    "                # Update and end the question span\n",
    "                question_span.update(\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    }\n",
    "                )\n",
    "                question_span.end()\n",
    "                \n",
    "                # Add score to the span (Langfuse v3 correct API)\n",
    "                langfuse_client.create_score(\n",
    "                    trace_id=trace_id,\n",
    "                    observation_id=question_span.id,\n",
    "                    name=\"question_evaluation\",\n",
    "                    value=1.0 if question_result['passed'] else 0.0,\n",
    "                    comment=question_result['reasoning'] or \"No reasoning provided\"\n",
    "                )\n",
    "                \n",
    "                span_count += 1\n",
    "        \n",
    "        # Update the main span\n",
    "        main_trace.update(\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update and end the main trace\n",
    "        main_trace.update_trace(\n",
    "                    output={\n",
    "                        \"PASSED\": passed_tests == total_tests\n",
    "                        }\n",
    "                )\n",
    "        \n",
    "        main_trace.end()\n",
    "        \n",
    "        # Add overall score to the trace (Langfuse v3 correct API)\n",
    "        overall_score = float(passed_tests) / float(total_tests) if total_tests > 0 else 0.0\n",
    "        langfuse_client.create_score(\n",
    "            trace_id=trace_id,\n",
    "            observation_id=main_trace.id,\n",
    "            name=\"overall_evaluation\",\n",
    "            value=overall_score,\n",
    "            comment=f\"Overall pass rate: {pass_rate}. {passed_tests}/{total_tests} tests passed.\"\n",
    "        )\n",
    "        \n",
    "        # Flush to ensure data is sent\n",
    "        langfuse_client.flush()\n",
    "        \n",
    "        print(f\"\\nüöÄ Evaluation results traced to Langfuse successfully!\")\n",
    "        print(f\"üìä Trace ID: {trace_id}\")\n",
    "        print(f\"üìà Total spans created: {span_count}\")\n",
    "        print(f\"üéØ Overall score: {overall_score:.2f}\")\n",
    "        print(f\"üîó Project: {config['langfuse']['project_name']} | Environment: {config['langfuse']['environment']}\")\n",
    "        \n",
    "        return trace_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error tracing to Langfuse: {str(e)}\")\n",
    "        print(\"üí° Make sure your Langfuse configuration is correct in config.json\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution \n",
    "\n",
    "Configure the evaluation parameters and run the evaluation with the fixed logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation...\n",
      "Evaluator Model: us.meta.llama4-maverick-17b-instruct-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Loading tests from: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/tests_structure.json\n",
      "\n",
      "=== Evaluating Test: analisis_criptomonedas ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¬øPuedes ayudarme a entender c√≥mo est√° desempe√±√°ndose el mercado cripto hoy?\n",
      "AGENT: Bas√°ndome en los datos actuales del mercado de criptomonedas, puedo ofrecerte un resumen:\n",
      "\n",
      "üîπ Principales caracter√≠sticas del mercado:\n",
      "- Bitcoin (BTC) est√° cotizando alrededor de $110,035\n",
      "- Ethereum (ETH) se encuentra cerca de $4,418\n",
      "- Otras criptomonedas muestran variaciones de precio entre $0.20 y $5,307\n",
      "\n",
      "El mercado muestra una diversidad de precios y parece estar relativamente estable. Sin embargo, para un an√°lisis m√°s detallado, te recomendar√≠a consultar fuentes especializadas que proporcionen informaci√≥n en tiempo real sobre tendencias y movimientos espec√≠ficos.\n",
      "\n",
      "¬øTe gustar√≠a que profundice en alg√∫n aspecto en particular del mercado cripto?\n",
      "Question 1 Status: PASSED\n",
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Pass Rate: 100.0%\n",
      "Tests Passed: 1/1\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "Test: analisis_criptomonedas\n",
      "Status: PASSED\n",
      "Result: All questions in the test passed - expected results observed.\n",
      "Reasoning: Q1: El agente proporciona un resumen del estado actual del mercado cripto, incluyendo precios de criptomonedas importantes y una evaluaci√≥n general de la estabilidad del mercado. Aunque sugiere fuentes especializadas para informaci√≥n m√°s detallada, cumple con el criterio esperado al ofrecer un an√°lisis inicial.\n",
      "Questions Evaluated: 1\n",
      "\n",
      "Question Details:\n",
      "  Q1: ‚úÖ PASSED\n",
      "    Question: ¬øPuedes ayudarme a entender c√≥mo est√° desempe√±√°ndose el mercado cripto hoy?...\n",
      "    Reasoning: El agente proporciona un resumen del estado actual del mercado cripto, incluyendo precios de criptomonedas importantes y una evaluaci√≥n general de la estabilidad del mercado. Aunque sugiere fuentes especializadas para informaci√≥n m√°s detallada, cumple con el criterio esperado al ofrecer un an√°lisis inicial.\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluation results saved to: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/evaluation_results/agent_evaluation_results_20250826_040917.csv\n",
      "üìà Total rows written: 1\n",
      "\n",
      "üîÑ Tracing evaluation results to Langfuse...\n",
      "üîó Langfuse client initialized successfully\n",
      "üìä Main trace created with ID: 01f3510fcdcec2a7dd5771a9e13983fa\n",
      "\n",
      "üöÄ Evaluation results traced to Langfuse successfully!\n",
      "üìä Trace ID: 01f3510fcdcec2a7dd5771a9e13983fa\n",
      "üìà Total spans created: 1\n",
      "üéØ Overall score: 1.00\n",
      "üîó Project: Financial-markets-super-agent | Environment: qa\n",
      "‚úÖ Langfuse tracing completed successfully!\n",
      "üîó Trace ID: 01f3510fcdcec2a7dd5771a9e13983fa\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation - one turn per question only.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"amazon.nova-premier-v1:0\", \"amazon.nova-pro-v1:0\", \"amazon.nova-lite-v1:0\", \"amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "\n",
    "    EVALUATOR_MODEL = \"us.meta.llama4-maverick-17b-instruct-v1:0\"  # Amazon Nova Pro model\n",
    "    AGENT_ID = config['agent']['agentId']  # Replace with your actual agent ID\n",
    "    AGENT_ALIAS_ID = config['agent']['agentAliasId']  # Replace with your actual alias ID\n",
    "    AWS_REGION = config['agent']['region']  # Replace with your region\n",
    "    TESTS_FILE = \"tests_structure.json\"\n",
    "\n",
    "    # Initialize evaluator (removed max_turns parameter - not needed anymore)\n",
    "    evaluator = CATAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "\n",
    "    evaluation_results = evaluator.run_evaluation(TESTS_FILE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Questions Evaluated: {result['turns']}\")\n",
    "        \n",
    "        # Show individual question results\n",
    "        if 'question_results' in result:\n",
    "            print(\"\\nQuestion Details:\")\n",
    "            for q_result in result['question_results']:\n",
    "                status = \"‚úÖ PASSED\" if q_result['passed'] else \"‚ùå FAILED\"\n",
    "                print(f\"  Q{q_result['question_number']}: {status}\")\n",
    "                print(f\"    Question: {q_result['question'][:100]}...\")\n",
    "                print(f\"    Reasoning: {q_result['reasoning']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = save_evaluation_results_to_csv(\n",
    "        evaluation_results=evaluation_results,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias=AGENT_ALIAS_ID\n",
    "    )\n",
    "    \n",
    "    # Trace results to Langfuse\n",
    "    try:\n",
    "        print(\"\\nüîÑ Tracing evaluation results to Langfuse...\")\n",
    "        trace_id = langfuse_evaluation_tracing(\n",
    "            evaluation_results=evaluation_results,\n",
    "            agent_id=AGENT_ID,\n",
    "            agent_alias=AGENT_ALIAS_ID,\n",
    "            evaluator_model=EVALUATOR_MODEL,\n",
    "            aws_region=AWS_REGION,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Langfuse tracing completed successfully!\")\n",
    "        print(f\"üîó Trace ID: {trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Langfuse tracing failed: {str(e)}\")\n",
    "        print(\"üìù Evaluation results are still saved to CSV file.\")\n",
    "        print(\"üí° Check your Langfuse configuration in config.json\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
