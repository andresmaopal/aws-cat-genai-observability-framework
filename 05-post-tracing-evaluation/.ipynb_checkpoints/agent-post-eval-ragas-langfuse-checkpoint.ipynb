{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Agent Evaluation (Ragas and LangFuse)\n",
    "\n",
    "This notebook provides a streamlined interface for evaluating agent performance using RAGAS metrics and LangFuse traces. All evaluation logic has been moved to `utils.py` and metrics are configured in `metrics_config.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:50:01.537020Z",
     "iopub.status.busy": "2025-10-23T18:50:01.536762Z",
     "iopub.status.idle": "2025-10-23T18:50:09.794401Z",
     "shell.execute_reply": "2025-10-23T18:50:09.793725Z",
     "shell.execute_reply.started": "2025-10-23T18:50:01.537000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "autogluon-common 1.4.0 requires pyarrow<21.0.0,>=7.0.0, but you have pyarrow 21.0.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.55.2 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.55.2 which is incompatible.\n",
      "langchain-aws 0.2.19 requires boto3>=1.37.24, but you have boto3 1.37.1 which is incompatible.\n",
      "mlflow 2.22.0 requires pyarrow<20,>=4.0.0, but you have pyarrow 21.0.0 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install ragas \"strands-agents==0.1.9\" \"strands-agents-tools==0.1.7\" \"langfuse==3.1.1\" pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:50:42.194519Z",
     "iopub.status.busy": "2025-10-23T18:50:42.194337Z",
     "iopub.status.idle": "2025-10-23T18:50:47.167597Z",
     "shell.execute_reply": "2025-10-23T18:50:47.167068Z",
     "shell.execute_reply.started": "2025-10-23T18:50:42.194501Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from utils import run_evaluation_pipeline, print_metric_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of Test Parameters\n",
    "\n",
    "Modify these parameters according to your evaluation needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:51:35.947151Z",
     "iopub.status.busy": "2025-10-23T18:51:35.946891Z",
     "iopub.status.idle": "2025-10-23T18:51:35.950568Z",
     "shell.execute_reply": "2025-10-23T18:51:35.950116Z",
     "shell.execute_reply.started": "2025-10-23T18:51:35.947130Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS - MODIFY AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# LangFuse Configuration\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-283af809-1820-4eb4-9a3c-05dbc8d68e9b\"  # Starts with sk-\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-837662dd-32fa-4331-9b86-609b62640939\"  # Starts with pk-\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\n",
    "\n",
    "# Evaluation Parameters\n",
    "LOOKBACK_HOURS = 24          # Hours to look back for traces\n",
    "BATCH_SIZE = 20              # Number of traces to process\n",
    "#LANGFUSE_TAGS = [\"Observability-Tutorial\"]  # Filter traces by tags (None for all)\n",
    "SAVE_CSV = True              # Save results to CSV files\n",
    "\n",
    "# Target LLM-as-Judge Model (from model_list.json)\n",
    "TARGET_MODEL = \"claude-3.7-sonnet\"  # Available models: claude-4-sonnet, nova-premier, etc.\n",
    "\n",
    "# File Paths\n",
    "METRICS_CONFIG_PATH = \"metrics_config.yaml\"\n",
    "MODEL_LIST_PATH = \"model_list.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LangFuse Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:51:36.952107Z",
     "iopub.status.busy": "2025-10-23T18:51:36.951843Z",
     "iopub.status.idle": "2025-10-23T18:51:36.956079Z",
     "shell.execute_reply": "2025-10-23T18:51:36.955549Z",
     "shell.execute_reply.started": "2025-10-23T18:51:36.952087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured for LangFuse host: https://us.cloud.langfuse.com\n",
      "Target evaluation model: claude-3.7-sonnet\n"
     ]
    }
   ],
   "source": [
    "# Setup OpenTelemetry endpoint\n",
    "otel_endpoint = os.environ[\"LANGFUSE_HOST\"] + \"/api/public/otel/v1/traces\"\n",
    "auth_token = base64.b64encode(f\"{os.environ[\"LANGFUSE_PUBLIC_KEY\"] }:{os.environ[\"LANGFUSE_SECRET_KEY\"]}\".encode()).decode()\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = otel_endpoint\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"Environment configured for LangFuse host: {os.environ[\"LANGFUSE_HOST\"]}\")\n",
    "print(f\"Target evaluation model: {TARGET_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation Pipeline\n",
    "\n",
    "Execute the complete evaluation pipeline with the configured parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAGAS evaluation pipeline...\n",
      "Configuration: 24h lookback, 20 traces, model: claude-3.7-sonnet\n",
      "Fetching traces from 2025-10-06 17:27:44.736659 to 2025-10-07 17:27:44.736659\n",
      "Fetched 3 traces\n",
      "Evaluating 3 multi_turn samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7476bf82d22744ef963f5df331bc8f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added score Tone of the agent Metric=0.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score Tool Usage Effectiveness=0.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score Tarea/Objetivo Cumplido=1.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score CI/CD Gate (Regresiones de Precisión)=1.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score Cumplimiento de Políticas=1.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score Answer Correctness=5.0 to trace be164c6c2a4d2d231167571522e7a042\n",
      "Added score Tone of the agent Metric=0.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score Tool Usage Effectiveness=0.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score Tarea/Objetivo Cumplido=0.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score CI/CD Gate (Regresiones de Precisión)=1.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score Cumplimiento de Políticas=0.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score Answer Correctness=5.0 to trace bcea59ab17e565c471badd32e4cbb7ad\n",
      "Added score Tone of the agent Metric=0.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Added score Tool Usage Effectiveness=0.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Added score Tarea/Objetivo Cumplido=1.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Added score CI/CD Gate (Regresiones de Precisión)=1.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Added score Cumplimiento de Políticas=0.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Added score Answer Correctness=5.0 to trace 078744bb74e88f037020e83f7064938e\n",
      "Results saved to evaluation_results/conversation_evaluation_20251007_172803.csv\n",
      "\n",
      "Evaluation pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "# Prepare LangFuse configuration\n",
    "langfuse_config = {\n",
    "    \"secret_key\": LANGFUSE_SECRET_KEY,\n",
    "    \"public_key\": LANGFUSE_PUBLIC_KEY,\n",
    "    \"host\": LANGFUSE_HOST\n",
    "}\n",
    "\n",
    "# Run the evaluation pipeline\n",
    "print(\"Starting RAGAS evaluation pipeline...\")\n",
    "print(f\"Configuration: {LOOKBACK_HOURS}h lookback, {BATCH_SIZE} traces, model: {TARGET_MODEL}\")\n",
    "\n",
    "results = run_evaluation_pipeline(\n",
    "    langfuse_config=langfuse_config,\n",
    "    model_name=TARGET_MODEL,\n",
    "    lookback_hours=LOOKBACK_HOURS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tags=LANGFUSE_TAGS,\n",
    "    save_csv=SAVE_CSV,\n",
    "    metrics_config_path=METRICS_CONFIG_PATH,\n",
    "    model_list_path=MODEL_LIST_PATH\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  MULTI-TURN CONVERSATION EVALUATION\n",
      "============================================================\n",
      "📊 Samples Evaluated: 3\n",
      "\n",
      "📈 METRIC SCORES SUMMARY\n",
      "----------------------------------------\n",
      "\n",
      "Tone of the agent Metric:\n",
      "  Mean: 0.000 | Min: 0.000 | Max: 0.000 | 🔴 POOR\n",
      "\n",
      "Tool Usage Effectiveness:\n",
      "  Mean: 0.000 | Min: 0.000 | Max: 0.000 | 🔴 POOR\n",
      "\n",
      "Tarea/Objetivo Cumplido:\n",
      "  Mean: 0.667 | Min: 0.000 | Max: 1.000 | 🟡 GOOD\n",
      "\n",
      "CI/CD Gate (Regresiones de Precisión):\n",
      "  Mean: 1.000 | Min: 1.000 | Max: 1.000 | 🟢 EXCELLENT\n",
      "\n",
      "Cumplimiento de Políticas:\n",
      "  Mean: 0.333 | Min: 0.000 | Max: 1.000 | 🔴 POOR\n",
      "\n",
      "Answer Correctness:\n",
      "  Mean: 5.000 | Min: 5.000 | Max: 5.000 | 🟢 EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "# Display results summary with configurable performance ranges\n",
    "if results:\n",
    "    \n",
    "    has_results = False\n",
    "    \n",
    "    # Performance range configuration - adjust as needed\n",
    "    # Examples: [0, 1] for 0-1 scale, [1, 5] for 1-5 scale\n",
    "    PERFORMANCE_RANGE = [0, 1]  # Change this to [1, 5] for 1-5 scale evaluation\n",
    "    \n",
    "    if \"conversation_results\" in results and results[\"conversation_results\"] is not None:\n",
    "        if not results[\"conversation_results\"].empty:\n",
    "            print_metric_summary(\n",
    "                results[\"conversation_results\"], \n",
    "                \"MULTI-TURN CONVERSATION EVALUATION\",\n",
    "                performance_range=PERFORMANCE_RANGE\n",
    "            )\n",
    "            has_results = True\n",
    "    \n",
    "    if \"single_turn_results\" in results and results[\"single_turn_results\"] is not None:\n",
    "        if not results[\"single_turn_results\"].empty:\n",
    "            print_metric_summary(\n",
    "                results[\"single_turn_results\"], \n",
    "                \"SINGLE-TURN EVALUATION\",\n",
    "                performance_range=PERFORMANCE_RANGE\n",
    "            )\n",
    "            has_results = True\n",
    "    \n",
    "    if not has_results:\n",
    "        print(\"\\n⚠️  No evaluation results available - check trace availability and configuration\")\n",
    "else:\n",
    "    print(\"\\n❌ No results returned from evaluation pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
