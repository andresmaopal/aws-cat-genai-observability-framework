{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAT Agent Evaluator \n",
    "\n",
    "This notebook provides a simplified agent evaluation framework that evaluates only the specific questions from the tests_structure.json file with exactly one turn per question.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- **Direct question evaluation from JSON file**\n",
    "- **One turn per question evaluation**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test JSON file (e.g., `tests_structure.json`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (3.1.6)\n",
      "Requirement already satisfied: jsonpath-ng in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: markdown-it-py in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2) (3.0.2)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.12/site-packages (from jsonpath-ng) (3.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent and LangFuse Configuration\n",
    "\n",
    "Load the configuration from config.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration loaded successfully!\n",
      "üè∑Ô∏è  Project: XXX\n",
      "üåç Environment: XXX\n",
      "ü§ñ Agent ID: CARG5UXPD9\n",
      "ü§ñ Agent Alias ID: 0RV9TBGQC4\n",
      "üîó Langfuse URL: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"üìã Configuration loaded successfully!\")\n",
    "print(f\"üè∑Ô∏è  Project: {config['langfuse']['project_name']}\")\n",
    "print(f\"üåç Environment: {config['langfuse']['environment']}\")\n",
    "print(f\"ü§ñ Agent ID: {config['agent']['agentId']}\")\n",
    "print(f\"ü§ñ Agent Alias ID: {config['agent']['agentAliasId']}\")\n",
    "print(f\"üîó Langfuse URL: {config['langfuse']['langfuse_api_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Langfuse Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Langfuse endpoint: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com/api/public/otel/v1/traces\n",
      "üè∑Ô∏è  Project: XXX, Environment: XXX\n"
     ]
    }
   ],
   "source": [
    "# Set up Langfuse configuration\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"DEPLOYMENT_ENVIRONMENT\"] = config[\"langfuse\"][\"environment\"]\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"üìä Langfuse endpoint: {os.environ['OTEL_EXPORTER_OTLP_ENDPOINT']}\")\n",
    "print(f\"üè∑Ô∏è  Project: {project_name}, Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for conversation handling and Bedrock agent communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles communication with different Bedrock model providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agent Evaluator Class \n",
    "\n",
    "The main evaluator class that evaluates only the specific questions from JSON file with one turn per question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator - FIXED to evaluate only JSON questions with one turn each.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\"):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags with improved error handling.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            try:\n",
    "                # Try exact match first\n",
    "                pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "                match = re.search(pattern, xml_data, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted = match.group(1).strip()\n",
    "                    content.append(extracted if extracted else None)\n",
    "                else:\n",
    "                    # Try case-insensitive match\n",
    "                    pattern = rf\"<{e.lower()}>(.*?)</{e.lower()}>\"\n",
    "                    match = re.search(pattern, xml_data.lower(), re.DOTALL)\n",
    "                    if match:\n",
    "                        # Find the original case version\n",
    "                        start_tag = f\"<{e.lower()}>\"\n",
    "                        end_tag = f\"</{e.lower()}>\"\n",
    "                        start_idx = xml_data.lower().find(start_tag)\n",
    "                        end_idx = xml_data.lower().find(end_tag)\n",
    "                        if start_idx != -1 and end_idx != -1:\n",
    "                            extracted = xml_data[start_idx + len(start_tag):end_idx].strip()\n",
    "                            content.append(extracted if extracted else None)\n",
    "                        else:\n",
    "                            content.append(None)\n",
    "                    else:\n",
    "                        content.append(None)\n",
    "            except Exception as ex:\n",
    "                print(f\"Warning: Error extracting {e} from XML: {ex}\")\n",
    "                content.append(None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_evaluation(self, expected_result: str, question: str, agent_response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of a single question-answer pair against expected result.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating an agent's response to a user question.\n",
    "\n",
    "Your job is to analyze the user question, agent response, and expected result to determine if the agent's response meets the expected criteria.\n",
    "\n",
    "You will classify the response into the following categories:\n",
    "\n",
    "- A: The agent's response meets or exceeds the expected result criteria.\n",
    "- B: The agent's response does not meet the expected result criteria.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter within <category> tags.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Here is the evaluation scenario:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<agent_response>\n",
    "{agent_response}\n",
    "</agent_response>\n",
    "\n",
    "<expected_result>\n",
    "{expected_result}\n",
    "</expected_result>\n",
    "\n",
    "Evaluate whether the agent's response meets the expected result criteria.\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with questions from JSON file - one turn per question.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        # Process each question as a separate turn\n",
    "        for i, (question, expected_result) in enumerate(zip(questions, expected_results)):\n",
    "            print(f\"\\nTurn {i + 1}\")\n",
    "            print(f\"USER: {question}\")\n",
    "            \n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(question)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(question, agent_response)\n",
    "            \n",
    "            # Evaluate this specific question-answer pair\n",
    "            eval_category, reasoning = self._generate_evaluation(expected_result, question, agent_response)\n",
    "            \n",
    "            question_passed = eval_category == \"A\"\n",
    "            question_result = {\n",
    "                \"question_number\": i + 1,\n",
    "                \"question\": question,\n",
    "                \"expected_result\": expected_result,\n",
    "                \"agent_response\": agent_response,\n",
    "                \"passed\": question_passed,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "            all_results.append(question_result)\n",
    "            \n",
    "            print(f\"Question {i + 1} Status: {'PASSED' if question_passed else 'FAILED'}\")\n",
    "        \n",
    "        # Overall test passes if all questions pass\n",
    "        overall_passed = all(result[\"passed\"] for result in all_results)\n",
    "        \n",
    "        if overall_passed:\n",
    "            overall_result = \"All questions in the test passed - expected results observed.\"\n",
    "        else:\n",
    "            failed_questions = [str(r[\"question_number\"]) for r in all_results if not r[\"passed\"]]\n",
    "            overall_result = f\"Test failed - questions {', '.join(failed_questions)} did not meet expected results.\"\n",
    "        \n",
    "        # Combine all reasoning\n",
    "        combined_reasoning = \" | \".join([f\"Q{r['question_number']}: {r['reasoning']}\" for r in all_results if r['reasoning']])\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": overall_passed,\n",
    "            \"result\": overall_result,\n",
    "            \"reasoning\": combined_reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns,\n",
    "            \"question_results\": all_results\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the JSON file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from: {tests_file_path}\")\n",
    "        \n",
    "        with open(tests_file_path, 'r') as f:\n",
    "            tests_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export Function\n",
    "\n",
    "This function exports evaluation results to a CSV file with detailed information for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results_to_csv(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                                   output_filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to a CSV file with detailed question-by-question information.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        output_filename: Optional custom filename. If None, generates timestamp-based name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"agent_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure .csv extension\n",
    "    if not output_filename.endswith('.csv'):\n",
    "        output_filename += '.csv'\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define evaluation results directory\n",
    "    results_dir = os.path.join(current_dir, \"evaluation_results\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define complete output path\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = [\n",
    "        'AGENT_ID',\n",
    "        'AGENT_ALIAS', \n",
    "        'TEST_NAME',\n",
    "        'QUESTION_NUMBER',\n",
    "        'QUESTION',\n",
    "        'EXPECTED_RESULT',\n",
    "        'AGENT_RESPONSE',\n",
    "        'QUESTION_PASSED',\n",
    "        'TEST_PASSED',\n",
    "        'REASONING'\n",
    "    ]\n",
    "    \n",
    "    # Write CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Process each test result\n",
    "        for result in evaluation_results['results']:\n",
    "            test_name = result['test_name']\n",
    "            test_passed = result['passed']\n",
    "            \n",
    "            # Process each question in the test\n",
    "            for question_result in result.get('question_results', []):\n",
    "                row = [\n",
    "                    agent_id,\n",
    "                    agent_alias,\n",
    "                    test_name,\n",
    "                    question_result['question_number'],\n",
    "                    question_result['question'],\n",
    "                    question_result['expected_result'],\n",
    "                    question_result['agent_response'],\n",
    "                    question_result['passed'],\n",
    "                    test_passed,\n",
    "                    question_result['reasoning'] or 'No reasoning provided'\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation results saved to: {output_path}\")\n",
    "    print(f\"üìà Total rows written: {sum(len(result.get('question_results', [])) for result in evaluation_results['results'])}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution \n",
    "\n",
    "Configure the evaluation parameters and run the evaluation with the fixed logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation - one turn per question only.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"amazon.nova-premier-v1:0\", \"amazon.nova-pro-v1:0\", \"amazon.nova-lite-v1:0\", \"amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "\n",
    "    EVALUATOR_MODEL = \"amazon.nova-pro-v1:0\"  # Amazon Nova Pro model\n",
    "    AGENT_ID = \"CARG5UXPD9\"  # Replace with your actual agent ID\n",
    "    AGENT_ALIAS_ID = \"0RV9TBGQC4\"  # Replace with your actual alias ID\n",
    "    AWS_REGION = \"us-east-1\"  # Replace with your region\n",
    "    TESTS_FILE = \"tests_structure.json\"\n",
    "\n",
    "    # Initialize evaluator (removed max_turns parameter - not needed anymore)\n",
    "    evaluator = CATAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"Starting FIXED Agent Evaluation...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "\n",
    "    evaluation_results = evaluator.run_evaluation(TESTS_FILE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Questions Evaluated: {result['turns']}\")\n",
    "        \n",
    "        # Show individual question results\n",
    "        if 'question_results' in result:\n",
    "            print(\"\\nQuestion Details:\")\n",
    "            for q_result in result['question_results']:\n",
    "                status = \"‚úÖ PASSED\" if q_result['passed'] else \"‚ùå FAILED\"\n",
    "                print(f\"  Q{q_result['question_number']}: {status}\")\n",
    "                print(f\"    Question: {q_result['question'][:100]}...\")\n",
    "                print(f\"    Reasoning: {q_result['reasoning']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = save_evaluation_results_to_csv(\n",
    "        evaluation_results=evaluation_results,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias=AGENT_ALIAS_ID\n",
    "    )\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
