{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Agent Evaluator\n",
    "\n",
    "This notebook provides a simplified agent evaluation framework extracted from the Agent Evaluation library. It supports multiple model providers through Amazon Bedrock including Anthropic Claude, Amazon Nova, Meta Llama, and OpenAI GPT models.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- Automated conversation evaluation\n",
    "- **Test file loading from current working directory**\n",
    "- **✅ FIXED: Amazon Nova compatibility with correct inferenceConfig**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0` ✅ **FIXED**\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test JSON file (e.g., `tests_structure.json`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation\n",
    "\n",
    "## Recent Fix\n",
    "- ✅ **Amazon Nova Final Fix**: Now uses correct `inferenceConfig` with `maxTokens` parameter\n",
    "- ❌ **Previous attempts**: `max_new_tokens` and `max_tokens` were both incorrect\n",
    "- ✅ **Current solution**: Uses proper Nova API format with nested configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (3.1.6)\n",
      "Requirement already satisfied: jsonpath-ng in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: markdown-it-py in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2) (3.0.2)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.12/site-packages (from jsonpath-ng) (3.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simplified Agent Evaluation Script\n",
    "Extracted from the Agent Evaluation framework to provide core evaluation functionality.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes. This supports:\n",
    "- **Anthropic**: Claude models via cross-region inference\n",
    "- **Amazon**: Nova models via cross-region inference (✅ **FIXED** with `inferenceConfig`)\n",
    "- **Meta**: Llama models via cross-region inference\n",
    "- **OpenAI**: GPT models via on-demand endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for:\n",
    "- **Conversation**: Captures multi-turn interactions between user and agent\n",
    "- **BedrockAgentTarget**: Handles communication with Amazon Bedrock agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles the communication with different Bedrock model providers. It:\n",
    "- **Builds request bodies** specific to each provider's API format\n",
    "- **Parses responses** from different model providers\n",
    "- **Supports all four providers**: Anthropic, Amazon, Meta, and OpenAI\n",
    "- **✅ FIXED Amazon Nova**: Uses correct `inferenceConfig` structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Agent Evaluator Class\n",
    "\n",
    "The main evaluator class that:\n",
    "- **Initializes** the evaluator with model configuration\n",
    "- **Creates model configs** for different providers (✅ **Nova FIXED** with `inferenceConfig`)\n",
    "- **Generates responses** using the evaluator model\n",
    "- **Evaluates conversations** and provides test results\n",
    "- **Uses current working directory** for test file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator based on the canonical evaluator logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\", max_turns: int = 10):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        self.max_turns = max_turns\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "            match = re.search(pattern, xml_data, re.DOTALL)\n",
    "            content.append(match.group(1).strip() if match else None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_initial_prompt(self, step: str) -> str:\n",
    "        \"\"\"Generate the initial prompt for the conversation.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer testing a conversational agent.\n",
    "\n",
    "Your job is to generate an initial prompt based on the provided step.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the initial prompt\n",
    "within <initial_prompt> tags.\"\"\"\n",
    "\n",
    "        prompt = f\"Generate an initial prompt for this step: {step}\"\n",
    "        \n",
    "        initial_prompt, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"initial_prompt\",\n",
    "        )\n",
    "        \n",
    "        return initial_prompt or step  # Fallback to original step if generation fails\n",
    "\n",
    "    def _generate_test_status(self, steps: List[str], conversation: Conversation) -> str:\n",
    "        \"\"\"Generate test status to determine if all steps have been attempted.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating a conversation between an USER and an AGENT.\n",
    "\n",
    "Your job is to analyze the conversation in <conversation> tags and a list of steps in <steps> tags.\n",
    "\n",
    "You will classify the conversation into the following categories:\n",
    "\n",
    "- A: All steps have been attempted in the conversation.\n",
    "- B: Not all steps have been attempted in the conversation.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter\n",
    "within <category> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(steps)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the steps and conversation:\n",
    "\n",
    "<steps>\n",
    "{steps_text}\n",
    "</steps>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\"\"\"\n",
    "\n",
    "        test_status, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return test_status\n",
    "\n",
    "    def _generate_evaluation(self, expected_results: List[str], conversation: Conversation) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of the conversation against expected results.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating a conversation between an USER and an AGENT.\n",
    "\n",
    "Your job is to analyze the conversation in <conversation> tags and a list of expected results\n",
    "in <expected_results> tags.\n",
    "\n",
    "You will classify the the conversation into the following categories:\n",
    "\n",
    "- A: All of the expected results can be observed in the conversation.\n",
    "- B: Not all of the expected results can be observed in the conversation.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter\n",
    "within <category> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        expected_results_text = \"\\n\".join([f\"{i+1}. {result}\" for i, result in enumerate(expected_results)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the expected results and conversation:\n",
    "\n",
    "<expected_results>\n",
    "{expected_results_text}\n",
    "</expected_results>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def _generate_user_response(self, steps: List[str], conversation: Conversation) -> str:\n",
    "        \"\"\"Generate the next user response based on steps and conversation history.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer testing a conversational agent.\n",
    "\n",
    "Your job is to generate the next user response based on the provided steps and conversation history.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the user response\n",
    "within <user_response> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(steps)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the steps and conversation history:\n",
    "\n",
    "<steps>\n",
    "{steps_text}\n",
    "</steps>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\n",
    "\n",
    "Generate the next appropriate user response to continue the conversation.\"\"\"\n",
    "\n",
    "        user_response, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"user_response\",\n",
    "        )\n",
    "        \n",
    "        return user_response\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with multiple questions.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        passed = False\n",
    "        result = \"Maximum turns reached.\"\n",
    "        reasoning = \"\"\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        while conversation.turns < self.max_turns:\n",
    "            if conversation.turns == 0:\n",
    "                # Start conversation with first question\n",
    "                user_input = questions[0] if questions else \"Hello\"\n",
    "                print(f\"\\nTurn {conversation.turns + 1}\")\n",
    "                print(f\"USER: {user_input}\")\n",
    "            else:\n",
    "                # Generate next user response\n",
    "                user_input = self._generate_user_response(questions, conversation)\n",
    "                print(f\"\\nTurn {conversation.turns + 1}\")\n",
    "                print(f\"USER: {user_input}\")\n",
    "\n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(user_input)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(user_input, agent_response)\n",
    "\n",
    "            # Check test status\n",
    "            test_status = self._generate_test_status(questions, conversation)\n",
    "            \n",
    "            if test_status == \"A\":  # All steps attempted\n",
    "                # Evaluate conversation\n",
    "                eval_category, reasoning = self._generate_evaluation(expected_results, conversation)\n",
    "                \n",
    "                if eval_category == \"A\":\n",
    "                    result = \"All of the expected results can be observed in the conversation.\"\n",
    "                    passed = True\n",
    "                else:\n",
    "                    result = \"Not all of the expected results can be observed in the conversation.\"\n",
    "                \n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": passed,\n",
    "            \"result\": result,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the JSON file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from: {tests_file_path}\")\n",
    "        \n",
    "        with open(tests_file_path, 'r') as f:\n",
    "            tests_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution\n",
    "\n",
    "Configure the evaluation parameters and run the evaluation:\n",
    "- **Model Selection**: Choose from Anthropic, Amazon, Meta, or OpenAI models\n",
    "- **Agent Configuration**: Set your Bedrock agent ID and alias\n",
    "- **Region Configuration**: Set your AWS region\n",
    "- **Test File**: Loads from current working directory\n",
    "- **Run Evaluation**: Execute the evaluation process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation...\n",
      "Evaluator Model: us.meta.llama4-maverick-17b-instruct-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Loading tests from: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/py_br_agent_evaluator/tests_structure.json\n",
      "\n",
      "=== Evaluating Test: simple_bitcoin_question ===\n",
      "\n",
      "Turn 1\n",
      "USER: what is the current bitcoin price?\n",
      "AGENT: Based on the current top cryptocurrencies data, Bitcoin's price is $114,607.84. Please note that cryptocurrency prices can fluctuate rapidly, so this is the most recent price available from the market data.\n",
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Pass Rate: 100.0%\n",
      "Tests Passed: 1/1\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "Test: simple_bitcoin_question\n",
      "Status: PASSED\n",
      "Result: All of the expected results can be observed in the conversation.\n",
      "Reasoning: The agent has provided the current Bitcoin price in USD as requested by the user, fulfilling the expected result. The conversation meets the criteria for category A because it satisfies the condition of providing an up-to-date Bitcoin price.\n",
      "Turns: 1\n",
      "\n",
      "Conversation:\n",
      "  USER: what is the current bitcoin price?\n",
      "  AGENT: Based on the current top cryptocurrencies data, Bitcoin's price is $114,607.84. Please note that cryptocurrency prices can fluctuate rapidly, so this is the most recent price available from the market data.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"amazon.nova-premier-v1:0\", \"amazon.nova-pro-v1:0\", \"amazon.nova-lite-v1:0\", \"amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "    \n",
    "    EVALUATOR_MODEL = \"us.meta.llama4-maverick-17b-instruct-v1:0\"  # Amazon Nova Pro model\n",
    "    AGENT_ID = \"CARG5UXPD9\"  # Replace with your actual agent ID\n",
    "    AGENT_ALIAS_ID = \"0RV9TBGQC4\"  # Replace with your actual alias ID\n",
    "    AWS_REGION = \"us-east-1\"  # Replace with your region\n",
    "    TESTS_FILE = \"tests_structure.json\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SimpleAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION,\n",
    "        max_turns=10\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "    \n",
    "    evaluation_results = evaluator.run_evaluation(TESTS_FILE)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Turns: {result['turns']}\")\n",
    "        \n",
    "        print(\"\\nConversation:\")\n",
    "        for sender, message in result['conversation']:\n",
    "            print(f\"  {sender}: {message}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
