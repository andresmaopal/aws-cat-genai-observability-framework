{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases Agent Evaluator (Amazon Bedrock Agents)\n",
    "\n",
    "This notebook provides a simplified agent evaluation framework that evaluates only the specific questions from YAML test files with exactly one turn per question.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- **Direct question evaluation from YAML file**\n",
    "- **One turn per question evaluation**\n",
    "- **YAML format support for better readability**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `amazon.nova-premier-v1:0`, `amazon.nova-pro-v1:0`, `amazon.nova-lite-v1:0`, `amazon.nova-micro-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test YAML file (e.g., `test_questions.yml`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip installboto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich langfuse==3.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import yaml\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# Langfuse SDK import for direct tracing\n",
    "from langfuse import Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent and LangFuse Configuration\n",
    "\n",
    "Load the configuration from config.json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration loaded successfully!\n",
      "üè∑Ô∏è  Project: Financial-markets-super-agent\n",
      "üåç Environment: qa\n",
      "ü§ñ Agent ID: CARG5UXPD9\n",
      "ü§ñ Agent Alias ID: 0RV9TBGQC4\n",
      "üîó Langfuse URL: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"üìã Configuration loaded successfully!\")\n",
    "print(f\"üè∑Ô∏è  Project: {config['langfuse']['project_name']}\")\n",
    "print(f\"üåç Environment: {config['langfuse']['environment']}\")\n",
    "print(f\"ü§ñ Agent ID: {config['agent']['agentId']}\")\n",
    "print(f\"ü§ñ Agent Alias ID: {config['agent']['agentAliasId']}\")\n",
    "print(f\"üîó Langfuse URL: {config['langfuse']['langfuse_api_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Langfuse Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Langfuse endpoint: http://langfu-loadb-ukoqudmq8a8v-2110705221.us-east-1.elb.amazonaws.com/api/public/otel/v1/traces\n",
      "üè∑Ô∏è  Project: Financial-markets-super-agent, Environment: qa\n"
     ]
    }
   ],
   "source": [
    "# Set up Langfuse configuration\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"LANGFUSE_TRACING_ENVIRONMENT\"] = 'qa'\n",
    "\n",
    "project_name = config[\"langfuse\"][\"project_name\"]\n",
    "environment = config[\"langfuse\"][\"environment\"]\n",
    "langfuse_public_key = config[\"langfuse\"][\"langfuse_public_key\"]\n",
    "langfuse_secret_key = config[\"langfuse\"][\"langfuse_secret_key\"]\n",
    "langfuse_api_url = config[\"langfuse\"][\"langfuse_api_url\"]\n",
    "\n",
    "# Create auth header\n",
    "auth_token = base64.b64encode(\n",
    "    f\"{langfuse_public_key}:{langfuse_secret_key}\".encode()\n",
    ").decode()\n",
    "\n",
    "# Set OpenTelemetry environment variables for Langfuse\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = f\"{langfuse_api_url}/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {auth_token}\"\n",
    "\n",
    "print(f\"üìä Langfuse endpoint: {os.environ['OTEL_EXPORTER_OTLP_ENDPOINT']}\")\n",
    "print(f\"üè∑Ô∏è  Project: {project_name}, Environment: {environment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for conversation handling and Bedrock agent communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles communication with different Bedrock model providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use system array format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Agent Evaluator Class \n",
    "\n",
    "The main evaluator class that evaluates only the specific questions from JSON file with one turn per question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CATAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator - FIXED to evaluate only JSON questions with one turn each.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\"):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"inferenceConfig\": {\n",
    "                        \"maxTokens\": 4000,\n",
    "                        \"temperature\": 0.0\n",
    "                    },\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags with improved error handling.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            try:\n",
    "                # Try exact match first\n",
    "                pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "                match = re.search(pattern, xml_data, re.DOTALL)\n",
    "                if match:\n",
    "                    extracted = match.group(1).strip()\n",
    "                    content.append(extracted if extracted else None)\n",
    "                else:\n",
    "                    # Try case-insensitive match\n",
    "                    pattern = rf\"<{e.lower()}>(.*?)</{e.lower()}>\"\n",
    "                    match = re.search(pattern, xml_data.lower(), re.DOTALL)\n",
    "                    if match:\n",
    "                        # Find the original case version\n",
    "                        start_tag = f\"<{e.lower()}>\"\n",
    "                        end_tag = f\"</{e.lower()}>\"\n",
    "                        start_idx = xml_data.lower().find(start_tag)\n",
    "                        end_idx = xml_data.lower().find(end_tag)\n",
    "                        if start_idx != -1 and end_idx != -1:\n",
    "                            extracted = xml_data[start_idx + len(start_tag):end_idx].strip()\n",
    "                            content.append(extracted if extracted else None)\n",
    "                        else:\n",
    "                            content.append(None)\n",
    "                    else:\n",
    "                        content.append(None)\n",
    "            except Exception as ex:\n",
    "                print(f\"Warning: Error extracting {e} from XML: {ex}\")\n",
    "                content.append(None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_evaluation(self, expected_result: str, question: str, agent_response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of a single question-answer pair against expected result.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating an agent's response to a user question.\n",
    "\n",
    "Your job is to analyze the user question, agent response, and expected result to determine if the agent's response meets the expected criteria.\n",
    "\n",
    "You will classify the response into the following categories:\n",
    "\n",
    "- A: The agent's response meets or exceeds the expected result criteria.\n",
    "- B: The agent's response does not meet the expected result criteria.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter within <category> tags. Evaluation Output must be in Spanish\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Here is the evaluation scenario:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<agent_response>\n",
    "{agent_response}\n",
    "</agent_response>\n",
    "\n",
    "<expected_result>\n",
    "{expected_result}\n",
    "</expected_result>\n",
    "\n",
    "Evaluate whether the agent's response meets the expected result criteria.\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with questions from JSON file - one turn per question.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        # Process each question as a separate turn\n",
    "        for i, (question, expected_result) in enumerate(zip(questions, expected_results)):\n",
    "            print(f\"\\nTurn {i + 1}\")\n",
    "            print(f\"USER: {question}\")\n",
    "            \n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(question)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(question, agent_response)\n",
    "            \n",
    "            # Evaluate this specific question-answer pair\n",
    "            eval_category, reasoning = self._generate_evaluation(expected_result, question, agent_response)\n",
    "            \n",
    "            question_passed = eval_category == \"A\"\n",
    "            question_result = {\n",
    "                \"question_number\": i + 1,\n",
    "                \"question\": question,\n",
    "                \"expected_result\": expected_result,\n",
    "                \"agent_response\": agent_response,\n",
    "                \"passed\": question_passed,\n",
    "                \"reasoning\": reasoning\n",
    "            }\n",
    "            all_results.append(question_result)\n",
    "            \n",
    "            print(f\"Question {i + 1} Status: {'PASSED' if question_passed else 'FAILED'}\")\n",
    "        \n",
    "        # Overall test passes if all questions pass\n",
    "        overall_passed = all(result[\"passed\"] for result in all_results)\n",
    "        \n",
    "        if overall_passed:\n",
    "            overall_result = \"All questions in the test passed - expected results observed.\"\n",
    "        else:\n",
    "            failed_questions = [str(r[\"question_number\"]) for r in all_results if not r[\"passed\"]]\n",
    "            overall_result = f\"Test failed - questions {', '.join(failed_questions)} did not meet expected results.\"\n",
    "        \n",
    "        # Combine all reasoning\n",
    "        combined_reasoning = \" | \".join([f\"Q{r['question_number']}: {r['reasoning']}\" for r in all_results if r['reasoning']])\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": overall_passed,\n",
    "            \"result\": overall_result,\n",
    "            \"reasoning\": combined_reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns,\n",
    "            \"question_results\": all_results\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the YAML file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from YAML: {tests_file_path}\")\n",
    "        \n",
    "        # Load YAML file instead of JSON\n",
    "        with open(tests_file_path, 'r', encoding='utf-8') as f:\n",
    "            tests_data = yaml.safe_load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Export Function\n",
    "\n",
    "This function exports evaluation results to a CSV file with detailed information for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results_to_csv(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                                   output_filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to a CSV file with detailed question-by-question information.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        output_filename: Optional custom filename. If None, generates timestamp-based name.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_filename = f\"agent_evaluation_results_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure .csv extension\n",
    "    if not output_filename.endswith('.csv'):\n",
    "        output_filename += '.csv'\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define evaluation results directory\n",
    "    results_dir = os.path.join(current_dir, \"evaluation_results\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define complete output path\n",
    "    output_path = os.path.join(results_dir, output_filename)\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = [\n",
    "        'AGENT_ID',\n",
    "        'AGENT_ALIAS', \n",
    "        'TEST_NAME',\n",
    "        'QUESTION_NUMBER',\n",
    "        'QUESTION',\n",
    "        'EXPECTED_RESULT',\n",
    "        'AGENT_RESPONSE',\n",
    "        'QUESTION_PASSED',\n",
    "        'TEST_PASSED',\n",
    "        'REASONING'\n",
    "    ]\n",
    "    \n",
    "    # Write CSV file\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Process each test result\n",
    "        for result in evaluation_results['results']:\n",
    "            test_name = result['test_name']\n",
    "            test_passed = result['passed']\n",
    "            \n",
    "            # Process each question in the test\n",
    "            for question_result in result.get('question_results', []):\n",
    "                row = [\n",
    "                    agent_id,\n",
    "                    agent_alias,\n",
    "                    test_name,\n",
    "                    question_result['question_number'],\n",
    "                    question_result['question'],\n",
    "                    question_result['expected_result'],\n",
    "                    question_result['agent_response'],\n",
    "                    question_result['passed'],\n",
    "                    test_passed,\n",
    "                    question_result['reasoning'] or 'No reasoning provided'\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation results saved to: {output_path}\")\n",
    "    print(f\"üìà Total rows written: {sum(len(result.get('question_results', [])) for result in evaluation_results['results'])}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Evaluation Tracing Function\n",
    "\n",
    "This function traces evaluation results to Langfuse v3 with a single trace containing agent metadata and individual spans for each test question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langfuse_evaluation_tracing(evaluation_results: Dict, agent_id: str, agent_alias: str, \n",
    "                               evaluator_model: str, aws_region: str, config: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Trace evaluation results to Langfuse v3 with a single trace containing agent metadata\n",
    "    and individual spans for each test question.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary containing evaluation results from run_evaluation()\n",
    "        agent_id: The Bedrock agent ID\n",
    "        agent_alias: The agent alias ID\n",
    "        evaluator_model: The model used for evaluation\n",
    "        aws_region: AWS region\n",
    "        config: Configuration dictionary with Langfuse credentials\n",
    "    \n",
    "    Returns:\n",
    "        str: Trace ID of the created trace\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize Langfuse client using config\n",
    "        langfuse_client = Langfuse(\n",
    "            secret_key=config['langfuse']['langfuse_secret_key'],\n",
    "            public_key=config['langfuse']['langfuse_public_key'],\n",
    "            host=config['langfuse']['langfuse_api_url']        )\n",
    "        \n",
    "        print(\"üîó Langfuse client initialized successfully\")\n",
    "        \n",
    "        # Initialize timestamp for metadata\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tests = evaluation_results['total_tests']\n",
    "        passed_tests = evaluation_results['passed_tests']\n",
    "        pass_rate = evaluation_results['pass_rate']\n",
    "        \n",
    "        # Create main trace with agent metadata and overall results (Langfuse v3 correct API)\n",
    "        main_trace = langfuse_client.start_span(\n",
    "            name=f\"Bedrock Agent Evaluation - {agent_id}\",\n",
    "            input={\n",
    "                \"agent_metadata\": {\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"agent_alias\": agent_alias,\n",
    "                    \"aws_region\": aws_region,\n",
    "                    \"evaluator_model\": evaluator_model,\n",
    "                    \"evaluation_timestamp\": timestamp.isoformat(),\n",
    "                    \"project_name\": config['langfuse']['project_name']                }\n",
    "            },\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            },\n",
    "            metadata={\n",
    "                \"evaluation_framework\": \"CAT Agent Evaluator\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"langfuse_version\": \"v3\",\n",
    "                \"project\": config['langfuse']['project_name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Get the actual trace ID from the span\n",
    "        trace_id = main_trace.trace_id\n",
    "        print(f\"üìä Main trace created with ID: {trace_id}\")\n",
    "\n",
    "        # Add tags to the trace using update_trace()\n",
    "        main_trace.update_trace(tags=[config['langfuse']['agentTag'], f\"BR-AgentID-{agent_id}\", f\"BR-AgentAlias-{agent_alias}\"])\n",
    "        \n",
    "        # Process each test result and create spans\n",
    "        span_count = 0\n",
    "        for test_result in evaluation_results['results']:\n",
    "            test_name = test_result['test_name']\n",
    "            test_passed = test_result['passed']\n",
    "            \n",
    "            # Create a span for each question in the test\n",
    "            for question_result in test_result.get('question_results', []):\n",
    "                question_id = f\"{test_name}_q{question_result['question_number']}\"\n",
    "                \n",
    "                # Create span for individual question (Langfuse v3 correct API)\n",
    "                question_span = main_trace.start_span(\n",
    "                    name=f\"Question: {test_name} - Q{question_result['question_number']}\",\n",
    "                    input={\n",
    "                        \"test_name\": test_name,\n",
    "                        \"question_id\": question_id,\n",
    "                        \"question\": question_result['question'],\n",
    "                        \"expected_result\": question_result['expected_result']\n",
    "                    },\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"question_number\": question_result['question_number'],\n",
    "                        \"test_passed\": test_passed,\n",
    "                        \"evaluation_category\": \"A\" if question_result['passed'] else \"B\",\n",
    "                        \"test_name\": test_name\n",
    "                    },\n",
    "                    level=\"DEFAULT\"\n",
    "                )\n",
    "                \n",
    "                # Update and end the question span\n",
    "                question_span.update(\n",
    "                    output={\n",
    "                        \"agent_response\": question_result['agent_response'],\n",
    "                        \"question_passed\": question_result['passed'],\n",
    "                        \"reasoning\": question_result['reasoning']\n",
    "                    }\n",
    "                )\n",
    "                question_span.end()\n",
    "                \n",
    "                # Add score to the span (Langfuse v3 correct API)\n",
    "                langfuse_client.create_score(\n",
    "                    trace_id=trace_id,\n",
    "                    observation_id=question_span.id,\n",
    "                    name=\"question_evaluation\",\n",
    "                    value=1.0 if question_result['passed'] else 0.0,\n",
    "                    comment=question_result['reasoning'] or \"No reasoning provided\"\n",
    "                )\n",
    "                \n",
    "                span_count += 1\n",
    "        \n",
    "        # Update the main span\n",
    "        main_trace.update(\n",
    "            output={\n",
    "                \"overall_test_result\": {\n",
    "                    \"pass_rate\": pass_rate,\n",
    "                    \"total_tests\": total_tests,\n",
    "                    \"passed_tests\": passed_tests,\n",
    "                    \"test_passed\": passed_tests == total_tests\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update and end the main trace\n",
    "        main_trace.update_trace(\n",
    "                    input={\n",
    "                        \"EVALUATION_FILE\": config['agent']['evaluation_file']\n",
    "                        },\n",
    "                    output={\n",
    "                        \"PASSED\": passed_tests == total_tests\n",
    "                        }\n",
    "                )\n",
    "        \n",
    "        main_trace.end()\n",
    "        \n",
    "        # Add overall score to the trace (Langfuse v3 correct API)\n",
    "        overall_score = float(passed_tests) / float(total_tests) if total_tests > 0 else 0.0\n",
    "        langfuse_client.create_score(\n",
    "            trace_id=trace_id,\n",
    "            observation_id=main_trace.id,\n",
    "            name=\"overall_evaluation\",\n",
    "            value=overall_score,\n",
    "            comment=f\"Overall pass rate: {pass_rate}. {passed_tests}/{total_tests} tests passed.\"\n",
    "        )\n",
    "        \n",
    "        # Flush to ensure data is sent\n",
    "        langfuse_client.flush()\n",
    "        \n",
    "        print(f\"\\nüöÄ Evaluation results traced to Langfuse successfully!\")\n",
    "        print(f\"üìä Trace ID: {trace_id}\")\n",
    "        print(f\"üìà Total spans created: {span_count}\")\n",
    "        print(f\"üéØ Overall score: {overall_score:.2f}\")\n",
    "        print(f\"üîó Project: {config['langfuse']['project_name']} | Environment: {config['langfuse']['environment']}\")\n",
    "        \n",
    "        return trace_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error tracing to Langfuse: {str(e)}\")\n",
    "        print(\"üí° Make sure your Langfuse configuration is correct in config.json\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution \n",
    "\n",
    "Configure the evaluation parameters and run the evaluation with the fixed logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation with YAML...\n",
      "Evaluator Model: us.amazon.nova-premier-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Test File: yfin_test_questions.yml (YAML format)\n",
      "Loading tests from YAML: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/yfin_test_questions.yml\n",
      "\n",
      "=== Evaluating Test: analisis_criptomonedas ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¬øPuedes ayudarme a entender c√≥mo est√° desempe√±√°ndose el mercado cripto hoy?\n",
      "AGENT: Bas√°ndome en los datos actuales del mercado de criptomonedas, puedo ofrecerte un resumen:\n",
      "\n",
      "üîπ Principales caracter√≠sticas:\n",
      "- Bitcoin (BTC) est√° alrededor de $109,865\n",
      "- Ethereum (ETH) se cotiza cerca de $4,272\n",
      "- Algunas monedas estables (como USDT) se mantienen cerca de $1\n",
      "\n",
      "El mercado muestra variedad de precios, desde monedas de centavos hasta aquellas con valores de miles de d√≥lares. Las criptomonedas principales mantienen una cotizaci√≥n estable.\n",
      "\n",
      "¬øTe gustar√≠a que profundice en alg√∫n aspecto espec√≠fico del mercado cripto?\n",
      "Question 1 Status: PASSED\n",
      "\n",
      "Turn 2\n",
      "USER: Bas√°ndo en esa informaci√≥n, ¬øcu√°les son las 3 mejores criptomonedas que tuvieron el mejor rendimiento hoy?\n",
      "AGENT: Bas√°ndome en los datos previos, las 3 criptomonedas con mejor desempe√±o son:\n",
      "\n",
      "1. Bitcoin (BTC): $109,865 - La criptomoneda l√≠der del mercado\n",
      "2. Ethereum (ETH): $4,272 - Segunda criptomoneda m√°s importante\n",
      "3. Una moneda con precio de $5,201 (posiblemente otra altcoin importante)\n",
      "\n",
      "Sin embargo, para darte una recomendaci√≥n m√°s precisa sobre rendimiento, necesitar√≠a informaci√≥n adicional sobre variaciones porcentuales y volumen de trading, que no est√°n incluidos en los datos actuales.\n",
      "\n",
      "¬øTe gustar√≠a que busque m√°s detalles espec√≠ficos sobre su rendimiento?\n",
      "Question 2 Status: FAILED\n",
      "\n",
      "Turn 3\n",
      "USER: Amplia informaci√≥n sobre la mejor criptomoneda de tu anterior analisis\n",
      "AGENT: üîπ Bitcoin (BTC) - An√°lisis Detallado:\n",
      "\n",
      "Caracter√≠sticas Principales:\n",
      "- Precio Actual: $109,865\n",
      "- Posici√≥n: L√≠der indiscutible del mercado de criptomonedas\n",
      "- Capitalizaci√≥n: La m√°s alta en el mercado cripto\n",
      "\n",
      "Aspectos Destacados:\n",
      "- Primera criptomoneda creada en el mundo\n",
      "- Considerada un activo de reserva digital\n",
      "- Ampliamente aceptada como inversi√≥n y medio de intercambio\n",
      "- Tecnolog√≠a blockchain que garantiza transacciones seguras\n",
      "\n",
      "Puntos Importantes:\n",
      "- Limitada a 21 millones de unidades totales\n",
      "- Altamente vol√°til pero con tendencia de valor a largo plazo\n",
      "- Respaldada por una comunidad global de inversores y desarrolladores\n",
      "\n",
      "Recomendaci√≥n: Siempre realizar una investigaci√≥n personal y consultar con asesores financieros antes de invertir.\n",
      "\n",
      "¬øDeseas que profundice en alg√∫n aspecto espec√≠fico de Bitcoin?\n",
      "Question 3 Status: PASSED\n",
      "\n",
      "=== Evaluating Test: noticias_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¬øCu√°les son las noticias m√°s importantes del mercado de valores de hoy?\n",
      "AGENT: Entiendo que est√°s interesado en las noticias m√°s importantes del mercado de valores de hoy. Para proporcionarte informaci√≥n precisa y relevante, necesito saber sobre qu√© acci√≥n o empresa en particular te gustar√≠a obtener noticias. \n",
      "\n",
      "¬øHay alguna empresa o sector espec√≠fico del mercado de valores sobre el que te gustar√≠a conocer las √∫ltimas noticias? Por ejemplo, ¬øte interesa saber sobre empresas tecnol√≥gicas como Apple (AAPL), gigantes del comercio electr√≥nico como Amazon (AMZN), o quiz√°s sobre √≠ndices generales del mercado?\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "=== Evaluating Test: informacion_acciones ===\n",
      "\n",
      "Turn 1\n",
      "USER: ¬øCu√°l es el precio actual de las acciones de Tesla y su variaci√≥n porcentual del d√≠a?\n",
      "AGENT: Lo siento, en este momento estoy experimentando dificultades t√©cnicas para obtener la informaci√≥n precisa sobre el precio de las acciones de Tesla. Los sistemas de informaci√≥n financiera parecen estar presentando un error moment√°neo.\n",
      "\n",
      "Lo que puedo recomendarte es:\n",
      "1. Verificar el precio actual en una plataforma financiera en l√≠nea\n",
      "2. Consultar tu aplicaci√≥n de inversiones preferida\n",
      "3. Revisar sitios web especializados como Yahoo Finance o Google Finance\n",
      "\n",
      "Si deseas, puedo intentar ayudarte de nuevo m√°s tarde para obtener los datos exactos de Tesla. ¬øTe gustar√≠a que lo intentemos en unos momentos?\n",
      "Question 1 Status: FAILED\n",
      "\n",
      "Turn 2\n",
      "USER: ¬øPuedes mostrarme el rendimiento semanal de NVIDIA comparado con el √≠ndice S&P 500?\n",
      "AGENT: Lo siento, actualmente no puedo proporcionarte los datos exactos sobre el rendimiento semanal de NVIDIA en comparaci√≥n con el S&P 500 debido a limitaciones t√©cnicas en el acceso a datos financieros actualizados.\n",
      "\n",
      "Lo que puedo compartir es la siguiente informaci√≥n:\n",
      "\n",
      "Para el S&P 500:\n",
      "- Nivel actual: 6,490.52 puntos\n",
      "- Cambio diario: +42.26 puntos (+0.66%)\n",
      "\n",
      "Para NVIDIA (NVDA):\n",
      "- Informaci√≥n general: Empresa l√≠der en el sector de semiconductores\n",
      "- Especializada en chips para inteligencia artificial y computaci√≥n gr√°fica\n",
      "- Ha mostrado un fuerte desempe√±o reciente en el mercado tecnol√≥gico\n",
      "\n",
      "Para obtener la informaci√≥n espec√≠fica sobre el rendimiento semanal comparativo que est√°s buscando, te recomendar√≠a consultar:\n",
      "1. Plataformas financieras como Yahoo Finance, Google Finance o Bloomberg\n",
      "2. Servicios de corretaje en l√≠nea\n",
      "3. Aplicaciones financieras especializadas\n",
      "\n",
      "¬øTe gustar√≠a que te ayude con otro tipo de an√°lisis o informaci√≥n financiera que pueda proporcionarte en este momento?\n",
      "Question 2 Status: FAILED\n",
      "\n",
      "============================================================\n",
      "YAML EVALUATION SUMMARY\n",
      "============================================================\n",
      "Pass Rate: 0.0%\n",
      "Tests Passed: 0/3\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "Test: analisis_criptomonedas\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 2 did not meet expected results.\n",
      "Reasoning: Q1: La respuesta del agente proporciona informaci√≥n actualizada sobre los precios de criptomonedas importantes como Bitcoin, Ethereum y monedas estables, lo que da una visi√≥n general del estado actual del mercado. Adem√°s, menciona la diversidad de precios en el mercado y la estabilidad de las criptomonedas principales, lo cual contribuye a entender el desempe√±o general. Finalmente, ofrece profundizar en aspectos espec√≠ficos, mostrando disposici√≥n para dar m√°s detalles. \n",
      "\n",
      "El resultado esperado era un an√°lisis del mercado cripto hoy, y aunque la respuesta no incluye an√°lisis t√©cnico detallado o tendencias espec√≠ficas, s√≠ ofrece una descripci√≥n b√°sica del estado del mercado que podr√≠a considerarse suficiente para una consulta general. Sin embargo, falta informaci√≥n sobre tendencias diarias, vol√∫menes de negociaci√≥n o factores influyentes, que podr√≠an enriquecer el an√°lisis. A√∫n as√≠, cumple con proporcionar una situaci√≥n general. | Q2: La pregunta del usuario solicita espec√≠ficamente las 3 criptomonedas con mejor rendimiento del d√≠a, lo que implica cambios porcentuales diarios. La respuesta del agente menciona tres criptomonedas pero no proporciona datos de rendimiento diario ni variaciones porcentuales, que son fundamentales para evaluar el \"mejor rendimiento hoy\". Aunque menciona precios actuales, esto no refleja el rendimiento diario. Adem√°s, la tercera opci√≥n es indeterminada (\"posiblemente otra altcoin\"), lo que muestra falta de precisi√≥n. El agente reconoce la falta de datos sobre variaciones porcentuales pero no resuelve la pregunta original. Por tanto, no cumple con el criterio esperado de identificar las criptomonedas con mejor rendimiento diario. | Q3: La pregunta del usuario solicita informaci√≥n ampliada sobre \"la mejor criptomoneda de tu anterior an√°lisis\". El agente responde con un an√°lisis detallado de Bitcoin, mencionando su precio, posici√≥n, capitalizaci√≥n, caracter√≠sticas hist√≥ricas, tecnol√≥gicas y de adopci√≥n. Aunque no especifica si Bitcoin era la criptomoneda destacada previamente, proporciona informaci√≥n sustancial que podr√≠a satisfacer la solicitud si Bitcoin fuera la criptomoneda analizada. La respuesta incluye datos espec√≠ficos y recomendaciones, cumpliendo con la necesidad de informaci√≥n detallada. Sin embargo, falta confirmaci√≥n expl√≠cita de que Bitcoin era el tema del an√°lisis anterior, pero la profundidad de la informaci√≥n probablemente compensa esta ausencia.\n",
      "Questions Evaluated: 3\n",
      "\n",
      "Question Details:\n",
      "  Q1: ‚úÖ PASSED\n",
      "    Question: ¬øPuedes ayudarme a entender c√≥mo est√° desempe√±√°ndose el mercado cripto hoy?...\n",
      "    Reasoning: La respuesta del agente proporciona informaci√≥n actualizada sobre los precios de criptomonedas importantes como Bitcoin, Ethereum y monedas estables, lo que da una visi√≥n general del estado actual del mercado. Adem√°s, menciona la diversidad de precios en el mercado y la estabilidad de las criptomonedas principales, lo cual contribuye a entender el desempe√±o general. Finalmente, ofrece profundizar en aspectos espec√≠ficos, mostrando disposici√≥n para dar m√°s detalles. \n",
      "\n",
      "El resultado esperado era un an√°lisis del mercado cripto hoy, y aunque la respuesta no incluye an√°lisis t√©cnico detallado o tendencias espec√≠ficas, s√≠ ofrece una descripci√≥n b√°sica del estado del mercado que podr√≠a considerarse suficiente para una consulta general. Sin embargo, falta informaci√≥n sobre tendencias diarias, vol√∫menes de negociaci√≥n o factores influyentes, que podr√≠an enriquecer el an√°lisis. A√∫n as√≠, cumple con proporcionar una situaci√≥n general.\n",
      "  Q2: ‚ùå FAILED\n",
      "    Question: Bas√°ndo en esa informaci√≥n, ¬øcu√°les son las 3 mejores criptomonedas que tuvieron el mejor rendimient...\n",
      "    Reasoning: La pregunta del usuario solicita espec√≠ficamente las 3 criptomonedas con mejor rendimiento del d√≠a, lo que implica cambios porcentuales diarios. La respuesta del agente menciona tres criptomonedas pero no proporciona datos de rendimiento diario ni variaciones porcentuales, que son fundamentales para evaluar el \"mejor rendimiento hoy\". Aunque menciona precios actuales, esto no refleja el rendimiento diario. Adem√°s, la tercera opci√≥n es indeterminada (\"posiblemente otra altcoin\"), lo que muestra falta de precisi√≥n. El agente reconoce la falta de datos sobre variaciones porcentuales pero no resuelve la pregunta original. Por tanto, no cumple con el criterio esperado de identificar las criptomonedas con mejor rendimiento diario.\n",
      "  Q3: ‚úÖ PASSED\n",
      "    Question: Amplia informaci√≥n sobre la mejor criptomoneda de tu anterior analisis...\n",
      "    Reasoning: La pregunta del usuario solicita informaci√≥n ampliada sobre \"la mejor criptomoneda de tu anterior an√°lisis\". El agente responde con un an√°lisis detallado de Bitcoin, mencionando su precio, posici√≥n, capitalizaci√≥n, caracter√≠sticas hist√≥ricas, tecnol√≥gicas y de adopci√≥n. Aunque no especifica si Bitcoin era la criptomoneda destacada previamente, proporciona informaci√≥n sustancial que podr√≠a satisfacer la solicitud si Bitcoin fuera la criptomoneda analizada. La respuesta incluye datos espec√≠ficos y recomendaciones, cumpliendo con la necesidad de informaci√≥n detallada. Sin embargo, falta confirmaci√≥n expl√≠cita de que Bitcoin era el tema del an√°lisis anterior, pero la profundidad de la informaci√≥n probablemente compensa esta ausencia.\n",
      "----------------------------------------\n",
      "\n",
      "Test: noticias_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1 did not meet expected results.\n",
      "Reasoning: Q1: La pregunta del usuario solicita directamente las noticias m√°s importantes del mercado de valores del d√≠a. El agente, en lugar de proporcionar informaci√≥n espec√≠fica sobre eventos actuales del mercado, pide aclaraciones sobre acciones o sectores espec√≠ficos. Aunque la solicitud de clarificaci√≥n podr√≠a ser √∫til en otros contextos, aqu√≠ no cumple con la expectativa expl√≠cita del usuario de recibir noticias actuales del mercado general. El agente no menciona ninguna noticia real ni proporciona datos del d√≠a, lo que hace que la respuesta no satisfaga el resultado esperado de ofrecer un resumen de noticias relevantes.\n",
      "Questions Evaluated: 1\n",
      "\n",
      "Question Details:\n",
      "  Q1: ‚ùå FAILED\n",
      "    Question: ¬øCu√°les son las noticias m√°s importantes del mercado de valores de hoy?...\n",
      "    Reasoning: La pregunta del usuario solicita directamente las noticias m√°s importantes del mercado de valores del d√≠a. El agente, en lugar de proporcionar informaci√≥n espec√≠fica sobre eventos actuales del mercado, pide aclaraciones sobre acciones o sectores espec√≠ficos. Aunque la solicitud de clarificaci√≥n podr√≠a ser √∫til en otros contextos, aqu√≠ no cumple con la expectativa expl√≠cita del usuario de recibir noticias actuales del mercado general. El agente no menciona ninguna noticia real ni proporciona datos del d√≠a, lo que hace que la respuesta no satisfaga el resultado esperado de ofrecer un resumen de noticias relevantes.\n",
      "----------------------------------------\n",
      "\n",
      "Test: informacion_acciones\n",
      "Status: FAILED\n",
      "Result: Test failed - questions 1, 2 did not meet expected results.\n",
      "Reasoning: Q1: La pregunta del usuario solicita espec√≠ficamente dos datos concretos: el precio actual de las acciones de Tesla y su variaci√≥n porcentual diaria. El resultado esperado requiere que el agente proporcione estos datos num√©ricos actualizados.\n",
      "\n",
      "La respuesta del agente explica que tiene dificultades t√©cnicas para obtener la informaci√≥n y sugiere tres alternativas para que el usuario encuentre la respuesta por su cuenta. Aunque ofrece recomendaciones √∫tiles y muestra empat√≠a, no proporciona el precio actual ni la variaci√≥n porcentual requerida. \n",
      "\n",
      "El agente no cumple con la entrega directa de los datos solicitados, que era el objetivo principal de la consulta. Aunque la explicaci√≥n sobre problemas t√©cnicos es v√°lida, no satisface completamente la necesidad inmediata del usuario. La respuesta no incluye los valores num√©ricos necesarios ni intenta ofrecer una soluci√≥n alternativa inmediata (como datos aproximados si estuvieran disponibles).\n",
      "\n",
      "Por lo tanto, la respuesta no cumple con los criterios esperados de proporcionar informaci√≥n financiera espec√≠fica y actualizada. | Q2: La pregunta del usuario solicita espec√≠ficamente el rendimiento semanal comparativo entre NVIDIA y el S&P 500. El resultado esperado requiere datos concretos de rendimiento porcentual y posiblemente gr√°ficos. \n",
      "\n",
      "La respuesta del agente explica que no puede proporcionar datos exactos debido a limitaciones t√©cnicas, ofrece informaci√≥n general sobre ambas entidades y sugiere fuentes alternativas. Aunque proporciona contexto √∫til y recomendaciones v√°lidas, no cumple con el requisito principal de mostrar el rendimiento semanal comparativo. \n",
      "\n",
      "El agente no menciona porcentajes semanales ni intenta aproximar la comparaci√≥n, lo que es esencial seg√∫n el resultado esperado. Aunque la respuesta es educada y ofrece alternativas, no satisface la solicitud directa del usuario. Por tanto, no cumple con los criterios esperados.\n",
      "Questions Evaluated: 2\n",
      "\n",
      "Question Details:\n",
      "  Q1: ‚ùå FAILED\n",
      "    Question: ¬øCu√°l es el precio actual de las acciones de Tesla y su variaci√≥n porcentual del d√≠a?...\n",
      "    Reasoning: La pregunta del usuario solicita espec√≠ficamente dos datos concretos: el precio actual de las acciones de Tesla y su variaci√≥n porcentual diaria. El resultado esperado requiere que el agente proporcione estos datos num√©ricos actualizados.\n",
      "\n",
      "La respuesta del agente explica que tiene dificultades t√©cnicas para obtener la informaci√≥n y sugiere tres alternativas para que el usuario encuentre la respuesta por su cuenta. Aunque ofrece recomendaciones √∫tiles y muestra empat√≠a, no proporciona el precio actual ni la variaci√≥n porcentual requerida. \n",
      "\n",
      "El agente no cumple con la entrega directa de los datos solicitados, que era el objetivo principal de la consulta. Aunque la explicaci√≥n sobre problemas t√©cnicos es v√°lida, no satisface completamente la necesidad inmediata del usuario. La respuesta no incluye los valores num√©ricos necesarios ni intenta ofrecer una soluci√≥n alternativa inmediata (como datos aproximados si estuvieran disponibles).\n",
      "\n",
      "Por lo tanto, la respuesta no cumple con los criterios esperados de proporcionar informaci√≥n financiera espec√≠fica y actualizada.\n",
      "  Q2: ‚ùå FAILED\n",
      "    Question: ¬øPuedes mostrarme el rendimiento semanal de NVIDIA comparado con el √≠ndice S&P 500?...\n",
      "    Reasoning: La pregunta del usuario solicita espec√≠ficamente el rendimiento semanal comparativo entre NVIDIA y el S&P 500. El resultado esperado requiere datos concretos de rendimiento porcentual y posiblemente gr√°ficos. \n",
      "\n",
      "La respuesta del agente explica que no puede proporcionar datos exactos debido a limitaciones t√©cnicas, ofrece informaci√≥n general sobre ambas entidades y sugiere fuentes alternativas. Aunque proporciona contexto √∫til y recomendaciones v√°lidas, no cumple con el requisito principal de mostrar el rendimiento semanal comparativo. \n",
      "\n",
      "El agente no menciona porcentajes semanales ni intenta aproximar la comparaci√≥n, lo que es esencial seg√∫n el resultado esperado. Aunque la respuesta es educada y ofrece alternativas, no satisface la solicitud directa del usuario. Por tanto, no cumple con los criterios esperados.\n",
      "----------------------------------------\n",
      "\n",
      "üìä Evaluation results saved to: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/cat_bedrock_agent_evaluator/evaluation_results/agent_evaluation_results_20250904_194436.csv\n",
      "üìà Total rows written: 6\n",
      "\n",
      "üîÑ Tracing evaluation results to Langfuse...\n",
      "üîó Langfuse client initialized successfully\n",
      "üìä Main trace created with ID: 871051eec2dd790a919530aa35ac01c6\n",
      "\n",
      "üöÄ Evaluation results traced to Langfuse successfully!\n",
      "üìä Trace ID: 871051eec2dd790a919530aa35ac01c6\n",
      "üìà Total spans created: 6\n",
      "üéØ Overall score: 0.00\n",
      "üîó Project: Financial-markets-super-agent | Environment: qa\n",
      "‚úÖ Langfuse tracing completed successfully!\n",
      "üîó Trace ID: 871051eec2dd790a919530aa35ac01c6\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation - one turn per question only.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"us.amazon.nova-premier-v1:0\", \"us.amazon.nova-pro-v1:0\", \"us.amazon.nova-lite-v1:0\", \"us.amazon.nova-micro-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "\n",
    "    EVALUATOR_MODEL = config['agent']['evaluator_model']  # Evaluation model cross-region endpoint \n",
    "    AGENT_ID = config['agent']['agentId']  #  your actual agent ID\n",
    "    AGENT_ALIAS_ID = config['agent']['agentAliasId']  # your actual alias ID\n",
    "    AWS_REGION = config['agent']['region']  # agent's region\n",
    "    EVAL_FILE = config['agent']['evaluation_file']  # YAML file with evaluation tests and questions\n",
    "\n",
    "    # Initialize evaluator (removed max_turns parameter - not needed anymore)\n",
    "    evaluator = CATAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation with YAML...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "    print(f\"Test File: {EVAL_FILE} (YAML format)\")\n",
    "\n",
    "    evaluation_results = evaluator.run_evaluation(EVAL_FILE)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"YAML EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Questions Evaluated: {result['turns']}\")\n",
    "        \n",
    "        # Show individual question results\n",
    "        if 'question_results' in result:\n",
    "            print(\"\\nQuestion Details:\")\n",
    "            for q_result in result['question_results']:\n",
    "                status = \"‚úÖ PASSED\" if q_result['passed'] else \"‚ùå FAILED\"\n",
    "                print(f\"  Q{q_result['question_number']}: {status}\")\n",
    "                print(f\"    Question: {q_result['question'][:100]}...\")\n",
    "                print(f\"    Reasoning: {q_result['reasoning']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = save_evaluation_results_to_csv(\n",
    "        evaluation_results=evaluation_results,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias=AGENT_ALIAS_ID\n",
    "    )\n",
    "    \n",
    "    # Trace results to Langfuse\n",
    "    try:\n",
    "        print(\"\\nüîÑ Tracing evaluation results to Langfuse...\")\n",
    "        trace_id = langfuse_evaluation_tracing(\n",
    "            evaluation_results=evaluation_results,\n",
    "            agent_id=AGENT_ID,\n",
    "            agent_alias=AGENT_ALIAS_ID,\n",
    "            evaluator_model=EVALUATOR_MODEL,\n",
    "            aws_region=AWS_REGION,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Langfuse tracing completed successfully!\")\n",
    "        print(f\"üîó Trace ID: {trace_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Langfuse tracing failed: {str(e)}\")\n",
    "        print(\"üìù Evaluation results are still saved to CSV file.\")\n",
    "        print(\"üí° Check your Langfuse configuration in config.json\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
