{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Agent Evaluator\n",
    "\n",
    "This notebook provides a simplified agent evaluation framework extracted from the Agent Evaluation library. It supports multiple model providers through Amazon Bedrock including Anthropic Claude, Amazon Nova, Meta Llama, and OpenAI GPT models.\n",
    "\n",
    "## Features\n",
    "- Multi-provider support (Anthropic, Amazon, Meta, OpenAI)\n",
    "- Cross-region inference endpoints\n",
    "- On-demand endpoints\n",
    "- Configurable AWS regions\n",
    "- Automated conversation evaluation\n",
    "- **Test file loading from current working directory**\n",
    "- **Fixed Amazon Nova model compatibility**\n",
    "\n",
    "## Available Models\n",
    "- **Anthropic**: `us.anthropic.claude-3-sonnet-20240229-v1:0`, `us.anthropic.claude-3-7-sonnet-20250219-v1:0`\n",
    "- **Amazon Nova**: `us.amazon.nova-premier-v1:0`, `us.amazon.nova-pro-v1:0`, `us.amazon.nova-lite-v1:0`\n",
    "- **Meta Llama**: `us.meta.llama4-maverick-17b-instruct-v1:0`, `us.meta.llama3-2-90b-instruct-v1:0`\n",
    "- **OpenAI**: `openai.gpt-oss-120b-1:0`, `openai.gpt-oss-20b-1:0`\n",
    "\n",
    "## Usage\n",
    "1. Place your test JSON file (e.g., `tests_structure.json`) in the same directory as this notebook\n",
    "2. Configure your model, agent, and region settings in the final cell\n",
    "3. Run all cells to execute the evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (8.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (3.1.6)\n",
      "Requirement already satisfied: jsonpath-ng in /opt/conda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: markdown-it-py in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.12/site-packages (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (14.0.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2) (3.0.2)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.12/site-packages (from jsonpath-ng) (3.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 click jinja2 jsonpath-ng markdown-it-py pydantic pyyaml rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary Python libraries for the agent evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simplified Agent Evaluation Script\n",
    "Extracted from the Agent Evaluation framework to provide core evaluation functionality.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Provider Configuration\n",
    "\n",
    "Define the supported model providers and configuration classes. This supports:\n",
    "- **Anthropic**: Claude models via cross-region inference\n",
    "- **Amazon**: Nova models via cross-region inference \n",
    "- **Meta**: Llama models via cross-region inference\n",
    "- **OpenAI**: GPT models via on-demand endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelProvider(Enum):\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    AMAZON = \"amazon\"\n",
    "    META = \"meta\"\n",
    "    OPENAI = \"openai\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BedrockModelConfig:\n",
    "    model_id: str\n",
    "    request_body: Dict\n",
    "    \n",
    "    @property\n",
    "    def provider(self) -> ModelProvider:\n",
    "        if \"anthropic\" in self.model_id:\n",
    "            return ModelProvider.ANTHROPIC\n",
    "        elif \"amazon\" in self.model_id:\n",
    "            return ModelProvider.AMAZON\n",
    "        elif \"meta\" in self.model_id:\n",
    "            return ModelProvider.META\n",
    "        elif \"openai\" in self.model_id:\n",
    "            return ModelProvider.OPENAI\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model ID: {self.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation and Target Agent Classes\n",
    "\n",
    "Define classes for:\n",
    "- **Conversation**: Captures multi-turn interactions between user and agent\n",
    "- **BedrockAgentTarget**: Handles communication with Amazon Bedrock agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Captures the interaction between a user and an agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        self.turns = 0\n",
    "\n",
    "    def add_turn(self, user_message: str, agent_response: str):\n",
    "        \"\"\"Record a turn in the conversation.\"\"\"\n",
    "        self.messages.extend([(\"USER\", user_message), (\"AGENT\", agent_response)])\n",
    "        self.turns += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.messages)\n",
    "\n",
    "\n",
    "class BedrockAgentTarget:\n",
    "    \"\"\"A target encapsulating an Amazon Bedrock agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_agent_id: str, bedrock_agent_alias_id: str, aws_region: str = \"us-east-1\"):\n",
    "        self.bedrock_agent_id = bedrock_agent_id\n",
    "        self.bedrock_agent_alias_id = bedrock_agent_alias_id\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.client = boto3.client(\"bedrock-agent-runtime\", region_name=aws_region)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke the target with a prompt.\"\"\"\n",
    "        response = self.client.invoke_agent(\n",
    "            agentId=self.bedrock_agent_id,\n",
    "            agentAliasId=self.bedrock_agent_alias_id,\n",
    "            sessionId=self.session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True,\n",
    "        )\n",
    "\n",
    "        stream = response[\"completion\"]\n",
    "        completion = \"\"\n",
    "        \n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                completion += chunk.get(\"bytes\").decode()\n",
    "\n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Request Handler\n",
    "\n",
    "This class handles the communication with different Bedrock model providers. It:\n",
    "- **Builds request bodies** specific to each provider's API format\n",
    "- **Parses responses** from different model providers\n",
    "- **Supports all four providers**: Anthropic, Amazon, Meta, and OpenAI\n",
    "- **âœ… Fixed Amazon Nova**: Uses correct `max_tokens` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockRequestHandler:\n",
    "    \"\"\"Static class for building requests to and receiving requests from Bedrock.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_request_body(request_body: Dict, model_config: BedrockModelConfig, \n",
    "                          system_prompt: str, prompt: str) -> Dict:\n",
    "        \"\"\"Build request body for different model providers.\"\"\"\n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            request_body[\"system\"] = system_prompt\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models use messages format\n",
    "            request_body[\"system\"] = [{\"text\": system_prompt}]\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"][0][\"content\"][0][\"text\"] = prompt\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models use prompt format\n",
    "            request_body[\"prompt\"] = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}\"\n",
    "                f\"<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}\"\n",
    "                \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models use messages format similar to OpenAI API\n",
    "            if \"messages\" in request_body:\n",
    "                request_body[\"messages\"] = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "        return request_body\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_completion_from_response(response: Dict, model_config: BedrockModelConfig) -> str:\n",
    "        \"\"\"Parse completion from different model provider responses.\"\"\"\n",
    "        response_body = response.get(\"body\").read()\n",
    "        response_json = json.loads(response_body)\n",
    "        \n",
    "        if model_config.provider == ModelProvider.ANTHROPIC:\n",
    "            completion = response_json[\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.AMAZON:\n",
    "            # Amazon Nova models return output in message format\n",
    "            completion = response_json[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        elif model_config.provider == ModelProvider.META:\n",
    "            # Meta Llama models return generation\n",
    "            completion = response_json[\"generation\"]\n",
    "        elif model_config.provider == ModelProvider.OPENAI:\n",
    "            # OpenAI models return choices with message content\n",
    "            completion = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {model_config.provider}\")\n",
    "            \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Agent Evaluator Class\n",
    "\n",
    "The main evaluator class that:\n",
    "- **Initializes** the evaluator with model configuration\n",
    "- **Creates model configs** for different providers (âœ… Nova compatibility fixed)\n",
    "- **Generates responses** using the evaluator model\n",
    "- **Evaluates conversations** and provides test results\n",
    "- **Uses current working directory** for test file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgentEvaluator:\n",
    "    \"\"\"Simplified agent evaluator based on the canonical evaluator logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator_model: str, agent_id: str, agent_alias_id: str, \n",
    "                 aws_region: str = \"us-east-1\", max_turns: int = 10):\n",
    "        self.evaluator_model = evaluator_model\n",
    "        self.agent_id = agent_id\n",
    "        self.agent_alias_id = agent_alias_id\n",
    "        self.aws_region = aws_region\n",
    "        self.max_turns = max_turns\n",
    "        \n",
    "        # Initialize Bedrock client for evaluator\n",
    "        self.bedrock_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "        \n",
    "        # Initialize target agent\n",
    "        self.target = BedrockAgentTarget(agent_id, agent_alias_id, aws_region)\n",
    "        \n",
    "        # Configure evaluator model based on provider\n",
    "        self.model_config = self._create_model_config(evaluator_model)\n",
    "\n",
    "    def _create_model_config(self, model_id: str) -> BedrockModelConfig:\n",
    "        \"\"\"Create model configuration based on the model provider.\"\"\"\n",
    "        if \"anthropic\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"amazon\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"\"}]}]\n",
    "                }\n",
    "            )\n",
    "        elif \"meta\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_gen_len\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"prompt\": \"\"\n",
    "                }\n",
    "            )\n",
    "        elif \"openai\" in model_id:\n",
    "            return BedrockModelConfig(\n",
    "                model_id=model_id,\n",
    "                request_body={\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"messages\": []  # Will be populated by build_request_body\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_id}\")\n",
    "\n",
    "    def _extract_content_from_xml(self, xml_data: str, element_names: List[str]) -> Tuple:\n",
    "        \"\"\"Extract content from XML tags.\"\"\"\n",
    "        content = []\n",
    "        for e in element_names:\n",
    "            pattern = rf\"<{e}>(.*?)</{e}>\"\n",
    "            match = re.search(pattern, xml_data, re.DOTALL)\n",
    "            content.append(match.group(1).strip() if match else None)\n",
    "        return tuple(content)\n",
    "\n",
    "    def _generate(self, system_prompt: str, prompt: str, output_xml_element: str) -> Tuple[str, str]:\n",
    "        \"\"\"Generate response using the evaluator model.\"\"\"\n",
    "        request_body = BedrockRequestHandler.build_request_body(\n",
    "            request_body=self.model_config.request_body.copy(),\n",
    "            model_config=self.model_config,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "\n",
    "        response = self.bedrock_client.invoke_model(\n",
    "            modelId=self.model_config.model_id, \n",
    "            body=json.dumps(request_body)\n",
    "        )\n",
    "\n",
    "        completion = BedrockRequestHandler.parse_completion_from_response(\n",
    "            response=response,\n",
    "            model_config=self.model_config\n",
    "        )\n",
    "\n",
    "        output, reasoning = self._extract_content_from_xml(\n",
    "            completion, [output_xml_element, \"thinking\"]\n",
    "        )\n",
    "\n",
    "        return output, reasoning\n",
    "\n",
    "    def _generate_initial_prompt(self, step: str) -> str:\n",
    "        \"\"\"Generate the initial prompt for the conversation.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer testing a conversational agent.\n",
    "\n",
    "Your job is to generate an initial prompt based on the provided step.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the initial prompt\n",
    "within <initial_prompt> tags.\"\"\"\n",
    "\n",
    "        prompt = f\"Generate an initial prompt for this step: {step}\"\n",
    "        \n",
    "        initial_prompt, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"initial_prompt\",\n",
    "        )\n",
    "        \n",
    "        return initial_prompt or step  # Fallback to original step if generation fails\n",
    "\n",
    "    def _generate_test_status(self, steps: List[str], conversation: Conversation) -> str:\n",
    "        \"\"\"Generate test status to determine if all steps have been attempted.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating a conversation between an USER and an AGENT.\n",
    "\n",
    "Your job is to analyze the conversation in <conversation> tags and a list of steps in <steps> tags.\n",
    "\n",
    "You will classify the conversation into the following categories:\n",
    "\n",
    "- A: All steps have been attempted in the conversation.\n",
    "- B: Not all steps have been attempted in the conversation.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter\n",
    "within <category> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(steps)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the steps and conversation:\n",
    "\n",
    "<steps>\n",
    "{steps_text}\n",
    "</steps>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\"\"\"\n",
    "\n",
    "        test_status, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return test_status\n",
    "\n",
    "    def _generate_evaluation(self, expected_results: List[str], conversation: Conversation) -> Tuple[str, str]:\n",
    "        \"\"\"Generate evaluation of the conversation against expected results.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer evaluating a conversation between an USER and an AGENT.\n",
    "\n",
    "Your job is to analyze the conversation in <conversation> tags and a list of expected results\n",
    "in <expected_results> tags.\n",
    "\n",
    "You will classify the the conversation into the following categories:\n",
    "\n",
    "- A: All of the expected results can be observed in the conversation.\n",
    "- B: Not all of the expected results can be observed in the conversation.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the category letter\n",
    "within <category> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        expected_results_text = \"\\n\".join([f\"{i+1}. {result}\" for i, result in enumerate(expected_results)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the expected results and conversation:\n",
    "\n",
    "<expected_results>\n",
    "{expected_results_text}\n",
    "</expected_results>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\"\"\"\n",
    "\n",
    "        evaluation, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"category\",\n",
    "        )\n",
    "        \n",
    "        return evaluation, reasoning\n",
    "\n",
    "    def _generate_user_response(self, steps: List[str], conversation: Conversation) -> str:\n",
    "        \"\"\"Generate the next user response based on steps and conversation history.\"\"\"\n",
    "        system_prompt = \"\"\"You are a quality assurance engineer testing a conversational agent.\n",
    "\n",
    "Your job is to generate the next user response based on the provided steps and conversation history.\n",
    "\n",
    "Please think hard about the response in <thinking> tags before providing only the user response\n",
    "within <user_response> tags.\"\"\"\n",
    "\n",
    "        conversation_text = \"\\n\".join([f\"{sender}: {message}\" for sender, message in conversation])\n",
    "        steps_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(steps)])\n",
    "        \n",
    "        prompt = f\"\"\"Here are the steps and conversation history:\n",
    "\n",
    "<steps>\n",
    "{steps_text}\n",
    "</steps>\n",
    "\n",
    "<conversation>\n",
    "{conversation_text}\n",
    "</conversation>\n",
    "\n",
    "Generate the next appropriate user response to continue the conversation.\"\"\"\n",
    "\n",
    "        user_response, reasoning = self._generate(\n",
    "            system_prompt=system_prompt,\n",
    "            prompt=prompt,\n",
    "            output_xml_element=\"user_response\",\n",
    "        )\n",
    "        \n",
    "        return user_response\n",
    "\n",
    "    def evaluate_test(self, test_name: str, questions: List[str], expected_results: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate a single test with multiple questions.\"\"\"\n",
    "        conversation = Conversation()\n",
    "        passed = False\n",
    "        result = \"Maximum turns reached.\"\n",
    "        reasoning = \"\"\n",
    "        \n",
    "        print(f\"\\n=== Evaluating Test: {test_name} ===\")\n",
    "        \n",
    "        while conversation.turns < self.max_turns:\n",
    "            if conversation.turns == 0:\n",
    "                # Start conversation with first question\n",
    "                user_input = questions[0] if questions else \"Hello\"\n",
    "                print(f\"\\nTurn {conversation.turns + 1}\")\n",
    "                print(f\"USER: {user_input}\")\n",
    "            else:\n",
    "                # Generate next user response\n",
    "                user_input = self._generate_user_response(questions, conversation)\n",
    "                print(f\"\\nTurn {conversation.turns + 1}\")\n",
    "                print(f\"USER: {user_input}\")\n",
    "\n",
    "            # Get agent response\n",
    "            agent_response = self.target.invoke(user_input)\n",
    "            print(f\"AGENT: {agent_response}\")\n",
    "            \n",
    "            # Add turn to conversation\n",
    "            conversation.add_turn(user_input, agent_response)\n",
    "\n",
    "            # Check test status\n",
    "            test_status = self._generate_test_status(questions, conversation)\n",
    "            \n",
    "            if test_status == \"A\":  # All steps attempted\n",
    "                # Evaluate conversation\n",
    "                eval_category, reasoning = self._generate_evaluation(expected_results, conversation)\n",
    "                \n",
    "                if eval_category == \"A\":\n",
    "                    result = \"All of the expected results can be observed in the conversation.\"\n",
    "                    passed = True\n",
    "                else:\n",
    "                    result = \"Not all of the expected results can be observed in the conversation.\"\n",
    "                \n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"test_name\": test_name,\n",
    "            \"passed\": passed,\n",
    "            \"result\": result,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"conversation\": [(sender, message) for sender, message in conversation.messages],\n",
    "            \"turns\": conversation.turns\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, tests_file: str) -> Dict:\n",
    "        \"\"\"Run evaluation on all tests from the JSON file.\"\"\"\n",
    "        # Get current working directory and construct full path\n",
    "        current_dir = os.getcwd()\n",
    "        tests_file_path = os.path.join(current_dir, tests_file)\n",
    "        \n",
    "        print(f\"Loading tests from: {tests_file_path}\")\n",
    "        \n",
    "        with open(tests_file_path, 'r') as f:\n",
    "            tests_data = json.load(f)\n",
    "        \n",
    "        results = []\n",
    "        total_tests = 0\n",
    "        passed_tests = 0\n",
    "        \n",
    "        for test_name, test_data in tests_data.items():\n",
    "            # Extract questions and expected results from the multi-turn structure\n",
    "            questions = []\n",
    "            expected_results = []\n",
    "            \n",
    "            for question_key in sorted(test_data.keys()):\n",
    "                if question_key.startswith('question_'):\n",
    "                    questions.append(test_data[question_key]['question'])\n",
    "                    expected_results.append(test_data[question_key]['expected_results'])\n",
    "            \n",
    "            # Run evaluation\n",
    "            test_result = self.evaluate_test(test_name, questions, expected_results)\n",
    "            results.append(test_result)\n",
    "            \n",
    "            total_tests += 1\n",
    "            if test_result['passed']:\n",
    "                passed_tests += 1\n",
    "        \n",
    "        # Calculate pass rate\n",
    "        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"pass_rate\": f\"{pass_rate:.1f}%\",\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"results\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Main Execution\n",
    "\n",
    "Configure the evaluation parameters and run the evaluation:\n",
    "- **Model Selection**: Choose from Anthropic, Amazon, Meta, or OpenAI models\n",
    "- **Agent Configuration**: Set your Bedrock agent ID and alias\n",
    "- **Region Configuration**: Set your AWS region\n",
    "- **Test File**: Loads from current working directory\n",
    "- **Run Evaluation**: Execute the evaluation process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Agent Evaluation...\n",
      "Evaluator Model: us.amazon.nova-premier-v1:0\n",
      "AWS Region: us-east-1\n",
      "Target Agent ID: CARG5UXPD9\n",
      "Target Agent Alias: 0RV9TBGQC4\n",
      "Loading tests from: /home/sagemaker-user/strands-langfuse/multi-agents-fmw/bedrock-agents-langfuse/py_br_agent_evaluator/tests_structure.json\n",
      "\n",
      "=== Evaluating Test: multi_domain_portfolio_analysis ===\n",
      "\n",
      "Turn 1\n",
      "USER: I'm considering diversifying my portfolio across different asset classes. Can you help me understand how tech stocks, treasury bonds, and Bitcoin are performing today?\n",
      "AGENT: Here's a comprehensive overview of how your selected asset classes are performing today:\n",
      "\n",
      "## Tech Stocks\n",
      "- The tech sector remains the largest in the market with a 29.99% market weight\n",
      "- Strong year-to-date (YTD) return of 29.99%\n",
      "- Performance indicator showing +11.83% \n",
      "- Tech continues to outperform many other sectors, reflecting ongoing innovation and digital transformation\n",
      "\n",
      "## Treasury Bonds\n",
      "Current yields:\n",
      "- 2-Year Treasury: 4.09%\n",
      "- 10-Year Treasury: 3.76%\n",
      "- 30-Year Treasury: 4.26%\n",
      "- Market Price Index: 112.14\n",
      "\n",
      "## Bitcoin\n",
      "- Current Price: $115,161.33\n",
      "\n",
      "Would you like more specific information about any particular companies within the tech sector, more detailed bond market analysis, or additional cryptocurrency insights to help with your portfolio diversification strategy?\n"
     ]
    },
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #/messages/0/content/0: extraneous key [type] is not permitted, please reformat your input and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Agent ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAGENT_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Agent Alias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAGENT_ALIAS_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTESTS_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 295\u001b[0m, in \u001b[0;36mSimpleAgentEvaluator.run_evaluation\u001b[0;34m(self, tests_file)\u001b[0m\n\u001b[1;32m    292\u001b[0m         expected_results\u001b[38;5;241m.\u001b[39mappend(test_data[question_key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected_results\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(test_result)\n\u001b[1;32m    298\u001b[0m total_tests \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 246\u001b[0m, in \u001b[0;36mSimpleAgentEvaluator.evaluate_test\u001b[0;34m(self, test_name, questions, expected_results)\u001b[0m\n\u001b[1;32m    243\u001b[0m conversation\u001b[38;5;241m.\u001b[39madd_turn(user_input, agent_response)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Check test status\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m test_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_test_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_status \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# All steps attempted\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Evaluate conversation\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     eval_category, reasoning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_evaluation(expected_results, conversation)\n",
      "Cell \u001b[0;32mIn[21], line 141\u001b[0m, in \u001b[0;36mSimpleAgentEvaluator._generate_test_status\u001b[0;34m(self, steps, conversation)\u001b[0m\n\u001b[1;32m    129\u001b[0m         steps_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(steps)])\n\u001b[1;32m    131\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHere are the steps and conversation:\u001b[39m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;124m<steps>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mconversation_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124m</conversation>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 141\u001b[0m         test_status, reasoning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_xml_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m test_status\n",
      "Cell \u001b[0;32mIn[21], line 79\u001b[0m, in \u001b[0;36mSimpleAgentEvaluator._generate\u001b[0;34m(self, system_prompt, prompt, output_xml_element)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate response using the evaluator model.\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m request_body \u001b[38;5;241m=\u001b[39m BedrockRequestHandler\u001b[38;5;241m.\u001b[39mbuild_request_body(\n\u001b[1;32m     73\u001b[0m     request_body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrequest_body\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m     74\u001b[0m     model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m     75\u001b[0m     system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[1;32m     76\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbedrock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m completion \u001b[38;5;241m=\u001b[39m BedrockRequestHandler\u001b[38;5;241m.\u001b[39mparse_completion_from_response(\n\u001b[1;32m     85\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m     86\u001b[0m     model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m output, reasoning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_content_from_xml(\n\u001b[1;32m     90\u001b[0m     completion, [output_xml_element, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinking\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #/messages/0/content/0: extraneous key [type] is not permitted, please reformat your input and try again."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the evaluation.\"\"\"\n",
    "    # Configuration - Update these values\n",
    "    # Available model options:\n",
    "    # - Anthropic: \"us.anthropic.claude-3-sonnet-20240229-v1:0\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    # - Amazon Nova: \"us.amazon.nova-premier-v1:0\", \"us.amazon.nova-pro-v1:0\", \"us.amazon.nova-lite-v1:0\"\n",
    "    # - Meta Llama: \"us.meta.llama4-maverick-17b-instruct-v1:0\", \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "    # - OpenAI: \"openai.gpt-oss-120b-1:0\", \"openai.gpt-oss-20b-1:0\"\n",
    "    \n",
    "    EVALUATOR_MODEL = \"us.amazon.nova-premier-v1:0\"  # On-demand endpoint\n",
    "    AGENT_ID = \"CARG5UXPD9\"  # Replace with your actual agent ID\n",
    "    AGENT_ALIAS_ID = \"0RV9TBGQC4\"  # Replace with your actual alias ID\n",
    "    AWS_REGION = \"us-east-1\"  # Replace with your region\n",
    "    TESTS_FILE = \"tests_structure.json\"\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SimpleAgentEvaluator(\n",
    "        evaluator_model=EVALUATOR_MODEL,\n",
    "        agent_id=AGENT_ID,\n",
    "        agent_alias_id=AGENT_ALIAS_ID,\n",
    "        aws_region=AWS_REGION,\n",
    "        max_turns=10\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"Starting Agent Evaluation...\")\n",
    "    print(f\"Evaluator Model: {EVALUATOR_MODEL}\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "    print(f\"Target Agent ID: {AGENT_ID}\")\n",
    "    print(f\"Target Agent Alias: {AGENT_ALIAS_ID}\")\n",
    "    \n",
    "    evaluation_results = evaluator.run_evaluation(TESTS_FILE)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Pass Rate: {evaluation_results['pass_rate']}\")\n",
    "    print(f\"Tests Passed: {evaluation_results['passed_tests']}/{evaluation_results['total_tests']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for result in evaluation_results['results']:\n",
    "        print(f\"\\nTest: {result['test_name']}\")\n",
    "        print(f\"Status: {'PASSED' if result['passed'] else 'FAILED'}\")\n",
    "        print(f\"Result: {result['result']}\")\n",
    "        print(f\"Reasoning: {result['reasoning']}\")\n",
    "        print(f\"Turns: {result['turns']}\")\n",
    "        \n",
    "        print(\"\\nConversation:\")\n",
    "        for sender, message in result['conversation']:\n",
    "            print(f\"  {sender}: {message}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
